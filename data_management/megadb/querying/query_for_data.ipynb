{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  # default is â€˜last_expr'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('/Users/siyuyang/Source/repos/GitHub_MSFT/CameraTraps')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import io\n",
    "from random import sample\n",
    "\n",
    "from tqdm import tqdm\n",
    "import azure.cosmos.cosmos_client as cosmos_client\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from PIL import Image\n",
    "\n",
    "from visualization import visualization_utils, visualize_megadb\n",
    "from data_management.annotations import annotation_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query for data\n",
    "\n",
    "This notebook demonstrates the workflow to compile desired sequences of images by querying metadata and downloading the images stored in blob storage.\n",
    "\n",
    "`COSMOS_ENDPOINT` and `COSMOS_KEY` need to be environment variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosmos DB config\n",
    "config = {\n",
    "    'ENDPOINT': os.environ.get('COSMOS_ENDPOINT'),\n",
    "    'PRIMARYKEY': os.environ.get('COSMOS_KEY')\n",
    "}\n",
    "\n",
    "# Initialize the Cosmos client\n",
    "client = cosmos_client.CosmosClient(url_connection=config['ENDPOINT'], auth={\n",
    "                                    'masterKey': config['PRIMARYKEY']})\n",
    "\n",
    "sequences_table = 'dbs/camera-trap/colls/sequences'  # database link + container link\n",
    "datasets_table = 'dbs/camera-trap/colls/datasets'\n",
    "\n",
    "options = {\n",
    "    'enableCrossPartitionQuery': True\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get the `datasets` table\n",
    "which records the location and access levels of each dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of results: 18\n",
      "CPU times: user 18.3 ms, sys: 2.9 ms, total: 21.2 ms\n",
      "Wall time: 450 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = {'query': '''SELECT * FROM datasets d'''}\n",
    "\n",
    "result_iterable = client.QueryItems(datasets_table, query, options)\n",
    "\n",
    "datasets = {i['dataset_name']:{k: v for k, v in i.items() if not k.startswith('_')} for i in iter(result_iterable)}\n",
    "\n",
    "print('Length of results:', len(datasets))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select image entries\n",
    "\n",
    "Example: top 1000 images from a given dataset with bounding boxes, selecting the file name and the dataset so we can plot the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of results: 10000\n",
      "CPU times: user 902 ms, sys: 79.1 ms, total: 981 ms\n",
      "Wall time: 16.3 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "query = {'query': '''\n",
    "SELECT TOP 10000 seq\n",
    "FROM sequences seq JOIN im IN seq.images \n",
    "WHERE ARRAY_LENGTH(im.bbox) > 0\n",
    "'''}\n",
    "\n",
    "options = {\n",
    "    'enableCrossPartitionQuery': True\n",
    "}\n",
    "\n",
    "result_iterable = client.QueryItems(sequences_table, query, options=options)\n",
    "# partition_key does not work here?\n",
    "\n",
    "results = [item['seq'] for item in iter(result_iterable)]\n",
    "\n",
    "print('Length of results:', len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_attachments': 'attachments/',\n",
       " '_etag': '\"1a00b5f9-0000-0500-0000-5dc5ee0e0000\"',\n",
       " '_rid': 'WjB+AJ4IhThfWQAAAAAAAA==',\n",
       " '_self': 'dbs/WjB+AA==/colls/WjB+AJ4IhTg=/docs/WjB+AJ4IhThfWQAAAAAAAA==/',\n",
       " '_ts': 1573252623,\n",
       " 'class': ['human'],\n",
       " 'dataset': 'wps_190624',\n",
       " 'id': '2da4e6e6-f576-4d3e-b549-c1d4d76d8fc4',\n",
       " 'images': [{'bbox': [{'bbox': [0.8723, 0.2887, 0.1276, 0.6912],\n",
       "     'category': 'person'}],\n",
       "   'class': ['__label_unavailable'],\n",
       "   'file': '377320b9-dbf4-45e3-b606-9d9c9d235fdf.jpg',\n",
       "   'image_id': '377320b9-dbf4-45e3-b606-9d9c9d235fdf'}],\n",
       " 'seq_id': 'dummy_ccc4c778a28941dd87fa9c79dba26c3f'}"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images and visualize labels\n",
    "\n",
    "For large batches, download using `multiprocessing.ThreadPool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 2\n",
    "sample_res = sample(results, sample_size)\n",
    "\n",
    "for seq in sample_res:\n",
    "    for im in seq['images']:\n",
    "        if 'bbox' not in im or len(im['bbox']) == 0:\n",
    "            continue\n",
    "    \n",
    "        dataset = seq['dataset']\n",
    "        storage_account = datasets[dataset]['storage_account']\n",
    "        storage_container = datasets[dataset]['container']\n",
    "        storage_sas_key = datasets[dataset]['container_sas_key']\n",
    "        path_prefix = datasets[dataset]['path_prefix']\n",
    "\n",
    "        blob_service = visualize_megadb.get_blob_service(datasets, dataset)\n",
    "        stream = io.BytesIO()\n",
    "        _ = blob_service.get_blob_to_stream(storage_container, os.path.join(path_prefix, im['file']), stream)\n",
    "        image = Image.open(stream)\n",
    "\n",
    "        visualization_utils.render_megadb_bounding_boxes(im['bbox'], image)\n",
    "        print('from dataset {}'.format(dataset))\n",
    "        image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
