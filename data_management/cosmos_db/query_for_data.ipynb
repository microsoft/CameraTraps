{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = 'all'  # default is â€˜last_expr'\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import io\n",
    "from random import sample\n",
    "\n",
    "from tqdm import tqdm\n",
    "import azure.cosmos.cosmos_client as cosmos_client\n",
    "from azure.storage.blob import BlockBlobService\n",
    "from PIL import Image\n",
    "\n",
    "from visualization import visualization_utils\n",
    "from data_management.annotations import annotation_constants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Query for data\n",
    "\n",
    "This notebook demonstrates the workflow to compile desired images by querying metadata using the database instance and downloading the images stored in blob storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cosmos DB config\n",
    "config = {\n",
    "    'ENDPOINT': os.environ.get('COSMOS_ENDPOINT'),\n",
    "    'PRIMARYKEY': os.environ.get('COSMOS_KEY')\n",
    "}\n",
    "\n",
    "# Initialize the Cosmos client\n",
    "client = cosmos_client.CosmosClient(url_connection=config['ENDPOINT'], auth={\n",
    "                                    'masterKey': config['PRIMARYKEY']})\n",
    "\n",
    "container_link = 'dbs/camera-trap/colls/images'  # database link + container link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('datasets.json') as f:\n",
    "    datasets_table = json.load(f)\n",
    "    \n",
    "# this is a json object with the account name as key, and the key to the account as value\n",
    "with open('blob_account_keys.json') as f:\n",
    "    blob_account_keys = json.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select image entries\n",
    "\n",
    "Example: top 1000 images from a given dataset with bounding boxes, selecting the file name and the dataset so we can plot the labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of results: 1000\n",
      "CPU times: user 104 ms, sys: 12.7 ms, total: 117 ms\n",
      "Wall time: 2.53 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "dataset_name = 'rspb_gola'\n",
    "# did not have species in many of these items\n",
    "query = {'query': '''\n",
    "SELECT TOP 1000 im.file_name, im.dataset, im.annotations.bbox, im.annotations.species\n",
    "FROM images im\n",
    "WHERE im.dataset = \"{}\" AND ARRAY_LENGTH(im.annotations.bbox) > 0\n",
    "'''.format(dataset_name)}\n",
    "\n",
    "options = {\n",
    "    'enableCrossPartitionQuery': True\n",
    "}\n",
    "\n",
    "result_iterable = client.QueryItems(container_link, query, options, partition_key='idfg')\n",
    "# if you want to restrict to one dataset, pass in partition_key=dataset\n",
    "\n",
    "results = []\n",
    "for item in iter(result_iterable):\n",
    "    results.append(item)\n",
    "\n",
    "print('Length of results:', len(results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(results)\n",
    "results[77]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download images and visualize labels\n",
    "\n",
    "For large batches, download using `multiprocessing.ThreadPool`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_size = 2\n",
    "sample_res = sample(results, sample_size)\n",
    "\n",
    "for im in sample_res:\n",
    "    dataset = im['dataset']\n",
    "    storage_account = datasets_table[dataset]['storage_account']\n",
    "    storage_container = datasets_table[dataset]['container']\n",
    "    path_prefix = datasets_table[dataset]['path_prefix']\n",
    "\n",
    "    print('Creating blob service')\n",
    "    blob_service = BlockBlobService(account_name=storage_account, account_key=blob_account_keys[storage_account])\n",
    "    print('Created')\n",
    "    stream = io.BytesIO()\n",
    "    _ = blob_service.get_blob_to_stream(storage_container, os.path.join(path_prefix, im['file_name']), stream)\n",
    "    print('Downloaded')\n",
    "    image = Image.open(stream)\n",
    "    print('Opened')\n",
    "    \n",
    "    boxes = []\n",
    "    classes = []\n",
    "    \n",
    "    for i in im['bbox']:\n",
    "        boxes.append(i['bbox_rel'])\n",
    "        classes.append(annotation_constants.bbox_category_name_to_id['animal'])\n",
    "\n",
    "    visualization_utils.render_iMerit_boxes(boxes, classes, image)\n",
    "    print('Visualized')\n",
    "    image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow]",
   "language": "python",
   "name": "conda-env-tensorflow-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
