{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d83bac3",
   "metadata": {},
   "source": [
    "# Managing a local MegaDetector batch\n",
    "\n",
    "This notebook represents an interactive process for running MegaDetector on large batches of images, including typical and optional postprocessing steps.  Everything after \"Merge results...\" is basically optional, and we typically do a mix of these optional steps, depending on the job.\n",
    "\n",
    "This notebook is auto-generated from manage_local_batch.py (a cell-delimited .py file that is used the same way, typically in Spyder or VS Code).    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62bd87f9",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44f9285c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import stat\n",
    "import time\n",
    "\n",
    "import humanfriendly\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from ai4eutils\n",
    "import ai4e_azure_utils\n",
    "import path_utils\n",
    "\n",
    "from detection.run_detector_batch import load_and_run_detector_batch, write_results_to_file\n",
    "from detection.run_detector import DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD\n",
    "\n",
    "from api.batch_processing.postprocessing.postprocess_batch_results import (\n",
    "    PostProcessingOptions, process_batch_results)\n",
    "from detection.run_detector import get_detector_version_from_filename\n",
    "\n",
    "max_task_name_length = 92\n",
    "\n",
    "# To specify a non-default confidence threshold for including detections in the .json file\n",
    "json_threshold = None\n",
    "\n",
    "# Turn warnings into errors if more than this many images are missing\n",
    "max_tolerable_failed_images = 100\n",
    "\n",
    "n_rendering_threads = 50\n",
    "\n",
    "use_image_queue = False\n",
    "\n",
    "# Only relevant when we're using a single GPU\n",
    "default_gpu_number = 0\n",
    "\n",
    "quiet_mode = True\n",
    "\n",
    "# Specify a target image size when running MD... strongly recommended to leave this at \"None\"\n",
    "image_size = None\n",
    "\n",
    "# Only relevant when running on CPU\n",
    "ncores = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cb4f76",
   "metadata": {},
   "source": [
    "## Constants I set per script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babea425",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = os.path.expanduser('~/data/organization/2021-12-24')\n",
    "\n",
    "organization_name_short = 'organization'\n",
    "# job_date = '2022-01-01'\n",
    "job_date = None\n",
    "assert job_date is not None and organization_name_short != 'organization'\n",
    "\n",
    "# Optional descriptor\n",
    "job_tag = None\n",
    "\n",
    "if job_tag is None:\n",
    "    job_description_string = ''\n",
    "else:\n",
    "    job_description_string = '-' + job_tag\n",
    "\n",
    "model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5a.0.0.pt')\n",
    "# model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')\n",
    "# model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')\n",
    "\n",
    "postprocessing_base = os.path.expanduser('~/postprocessing')\n",
    "\n",
    "# Number of jobs to split data into, typically equal to the number of available GPUs\n",
    "n_jobs = 2\n",
    "n_gpus = 2\n",
    "\n",
    "# Only used to print out a time estimate\n",
    "if ('v5') in model_file:\n",
    "    gpu_images_per_second = 10\n",
    "else:\n",
    "    gpu_images_per_second = 2.9\n",
    "\n",
    "checkpoint_frequency = 10000\n",
    "\n",
    "base_task_name = organization_name_short + '-' + job_date + job_description_string + '-' + get_detector_version_from_filename(model_file)\n",
    "base_output_folder_name = os.path.join(postprocessing_base,organization_name_short)\n",
    "os.makedirs(base_output_folder_name,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f298866",
   "metadata": {},
   "source": [
    "## Derived variables, path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_base = os.path.join(base_output_folder_name, base_task_name)\n",
    "combined_api_output_folder = os.path.join(filename_base, 'combined_api_outputs')\n",
    "postprocessing_output_folder = os.path.join(filename_base, 'preview')\n",
    "\n",
    "os.makedirs(filename_base, exist_ok=True)\n",
    "os.makedirs(combined_api_output_folder, exist_ok=True)\n",
    "os.makedirs(postprocessing_output_folder, exist_ok=True)\n",
    "\n",
    "if input_path.endswith('/'):\n",
    "    input_path = input_path[0:-1]\n",
    "\n",
    "print('Output folder:\\n{}'.format(filename_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aa44f0b",
   "metadata": {},
   "source": [
    "## Enumerate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d2ef719",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = path_utils.find_images(input_path,recursive=True)\n",
    "\n",
    "print('Enumerated {} image files in {}'.format(len(all_images),input_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d478e185",
   "metadata": {},
   "source": [
    "## Divide images into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeea3e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(L, n):\n",
    "    k, m = divmod(len(L), n)\n",
    "    return list(L[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "folder_chunks = split_list(all_images,n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9e82046",
   "metadata": {},
   "source": [
    "## Estimate total time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a4f331",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = len(all_images)\n",
    "execution_seconds = n_images / gpu_images_per_second\n",
    "wallclock_seconds = execution_seconds / n_gpus\n",
    "print('Expected time: {}'.format(humanfriendly.format_timespan(wallclock_seconds)))\n",
    "\n",
    "seconds_per_chunk = len(folder_chunks[0]) / gpu_images_per_second\n",
    "print('Expected time per chunk: {}'.format(humanfriendly.format_timespan(seconds_per_chunk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98bb04f1",
   "metadata": {},
   "source": [
    "## Write file lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1417c1bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_info = []\n",
    "\n",
    "for i_chunk,chunk_list in enumerate(folder_chunks):\n",
    "\n",
    "    chunk_fn = os.path.join(filename_base,'chunk{}.json'.format(str(i_chunk).zfill(3)))\n",
    "    task_info.append({'id':i_chunk,'input_file':chunk_fn})\n",
    "    ai4e_azure_utils.write_list_to_file(chunk_fn, chunk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f53237",
   "metadata": {},
   "source": [
    "## Generate commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84ca8187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_task = 0; task = task_info[i_task]\n",
    "for i_task,task in enumerate(task_info):\n",
    "\n",
    "    chunk_file = task['input_file']\n",
    "    output_fn = chunk_file.replace('.json','_results.json')\n",
    "\n",
    "    task['output_file'] = output_fn\n",
    "\n",
    "    if n_jobs > 1:\n",
    "        gpu_number = i_task % n_gpus\n",
    "    else:\n",
    "        gpu_number = default_gpu_number\n",
    "\n",
    "    cuda_string = f'CUDA_VISIBLE_DEVICES={gpu_number}'\n",
    "\n",
    "    checkpoint_frequency_string = ''\n",
    "    checkpoint_path_string = ''\n",
    "    checkpoint_filename = chunk_file.replace('.json','_checkpoint.json')\n",
    "\n",
    "    if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "        checkpoint_frequency_string = f'--checkpoint_frequency {checkpoint_frequency}'\n",
    "        checkpoint_path_string = '--checkpoint_path \"{}\"'.format(checkpoint_filename)\n",
    "\n",
    "    use_image_queue_string = ''\n",
    "    if (use_image_queue):\n",
    "        use_image_queue_string = '--use_image_queue'\n",
    "\n",
    "    ncores_string = ''\n",
    "    if (ncores > 1):\n",
    "        ncores_string = '--ncores {}'.format(ncores)\n",
    "\n",
    "    quiet_string = ''\n",
    "    if quiet_mode:\n",
    "        quiet_string = '--quiet'\n",
    "\n",
    "    image_size_string = ''\n",
    "    if image_size is not None:\n",
    "        image_size_string = '--image_size {}'.format(image_size)\n",
    "\n",
    "    # Generate the script to run MD\n",
    "\n",
    "    cmd = f'{cuda_string} python run_detector_batch.py \"{model_file}\" \"{chunk_file}\" \"{output_fn}\" {checkpoint_frequency_string} {checkpoint_path_string} {use_image_queue_string} {ncores_string} {quiet_string} {image_size_string}'\n",
    "\n",
    "    cmd_file = os.path.join(filename_base,'run_chunk_{}_gpu_{}.sh'.format(str(i_task).zfill(2),\n",
    "                            str(gpu_number).zfill(2)))\n",
    "\n",
    "    with open(cmd_file,'w') as f:\n",
    "        f.write(cmd + '\\n')\n",
    "\n",
    "    st = os.stat(cmd_file)\n",
    "    os.chmod(cmd_file, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "    task['command'] = cmd\n",
    "    task['command_file'] = cmd_file\n",
    "\n",
    "    # Generate the script to resume from the checkpoint\n",
    "\n",
    "    resume_string = ' --resume_from_checkpoint \"{}\"'.format(checkpoint_filename)\n",
    "    resume_cmd = cmd + resume_string\n",
    "    resume_cmd_file = os.path.join(filename_base,'resume_chunk_{}_gpu_{}.sh'.format(str(i_task).zfill(2),\n",
    "                            str(gpu_number).zfill(2)))\n",
    "\n",
    "    with open(resume_cmd_file,'w') as f:\n",
    "        f.write(resume_cmd + '\\n')\n",
    "\n",
    "    st = os.stat(resume_cmd_file)\n",
    "    os.chmod(resume_cmd_file, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "    task['resume_command'] = resume_cmd\n",
    "    task['resume_command_file'] = resume_cmd_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "866125ae",
   "metadata": {},
   "source": [
    "## Run the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b97f3a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I strongly prefer to manually run the scripts we just generated, but this cell demonstrates\n",
    "how one would invoke run_detector_batch programmatically.  Normally when I run manually on\n",
    "a multi-GPU machine, I run the scripts in N separate shells, one for each GPU.  This programmatic\n",
    "approach does not yet know how to do something like that, so all chunks will run serially.\n",
    "This is a no-op if you're on a single-GPU machine.\n",
    "\"\"\"\n",
    "\n",
    "if False:\n",
    "\n",
    "    #%%% Run the tasks (commented out)\n",
    "\n",
    "    # i_task = 0; task = task_info[i_task]\n",
    "    for i_task,task in enumerate(task_info):\n",
    "\n",
    "        chunk_file = task['input_file']\n",
    "        output_fn = task['output_file']\n",
    "\n",
    "        checkpoint_filename = chunk_file.replace('.json','_checkpoint.json')\n",
    "\n",
    "        if json_threshold is not None:\n",
    "            confidence_threshold = json_threshold\n",
    "        else:\n",
    "            confidence_threshold = DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD\n",
    "\n",
    "        if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "            cp_freq_arg = checkpoint_frequency\n",
    "        else:\n",
    "            cp_freq_arg = -1\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = load_and_run_detector_batch(model_file=model_file,\n",
    "                                              image_file_names=chunk_file,\n",
    "                                              checkpoint_path=checkpoint_filename,\n",
    "                                              confidence_threshold=confidence_threshold,\n",
    "                                              checkpoint_frequency=cp_freq_arg,\n",
    "                                              results=None,\n",
    "                                              n_cores=ncores,\n",
    "                                              use_image_queue=use_image_queue,\n",
    "                                              quiet=quiet_mode,\n",
    "                                              image_size=image_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print('Task {}: finished inference for {} images in {}'.format(\n",
    "            i_task, len(results),humanfriendly.format_timespan(elapsed)))\n",
    "\n",
    "        # This will write absolute paths to the file, we'll fix this later\n",
    "        write_results_to_file(results, output_fn, detector_file=model_file)\n",
    "\n",
    "        if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "            if os.path.isfile(checkpoint_filename):\n",
    "                os.remove(checkpoint_filename)\n",
    "                print('Deleted checkpoint file {}'.format(checkpoint_filename))\n",
    "\n",
    "    # ...for each chunk\n",
    "\n",
    "# ...if False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd04073",
   "metadata": {},
   "source": [
    "## Load results, look for failed or missing images in each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335a20c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_failures = 0\n",
    "\n",
    "# i_task = 0; task = task_info[i_task]\n",
    "for i_task,task in enumerate(task_info):\n",
    "\n",
    "    chunk_file = task['input_file']\n",
    "    output_file = task['output_file']\n",
    "\n",
    "    with open(chunk_file,'r') as f:\n",
    "        task_images = json.load(f)\n",
    "    with open(output_file,'r') as f:\n",
    "        task_results = json.load(f)\n",
    "\n",
    "    task_images_set = set(task_images)\n",
    "    filename_to_results = {}\n",
    "\n",
    "    n_task_failures = 0\n",
    "\n",
    "    # im = task_results['images'][0]\n",
    "    for im in task_results['images']:\n",
    "        assert im['file'].startswith(input_path)\n",
    "        assert im['file'] in task_images_set\n",
    "        filename_to_results[im['file']] = im\n",
    "        if 'failure' in im:\n",
    "            assert im['failure'] is not None\n",
    "            n_task_failures += 1\n",
    "\n",
    "    task['n_failures'] = n_task_failures\n",
    "    task['results'] = task_results\n",
    "\n",
    "    for fn in task_images:\n",
    "        assert fn in filename_to_results\n",
    "\n",
    "    n_total_failures += n_task_failures\n",
    "\n",
    "# ...for each task\n",
    "\n",
    "assert n_total_failures < max_tolerable_failed_images,\\\n",
    "    '{} failures (max tolerable set to {})'.format(n_total_failures,\n",
    "                                                   max_tolerable_failed_images)\n",
    "\n",
    "print('Processed all {} images with {} failures'.format(\n",
    "    len(all_images),n_total_failures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69634ed6",
   "metadata": {},
   "source": [
    "## Merge results files and make images relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5994a4a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "combined_results = None\n",
    "\n",
    "for i_task,task in enumerate(task_info):\n",
    "\n",
    "    if i_task == 0:\n",
    "        combined_results = copy.deepcopy(task['results'])\n",
    "        combined_results['images'] = copy.deepcopy(task['results']['images'])\n",
    "        continue\n",
    "    task_results = task['results']\n",
    "    assert task_results['info']['format_version'] == combined_results['info']['format_version']\n",
    "    assert task_results['detection_categories'] == combined_results['detection_categories']\n",
    "    combined_results['images'].extend(copy.deepcopy(task_results['images']))\n",
    "\n",
    "assert len(combined_results['images']) == len(all_images), \\\n",
    "    'Expected {} images in combined results, found {}'.format(\n",
    "        len(all_images),len(combined_results['images']))\n",
    "\n",
    "result_filenames = [im['file'] for im in combined_results['images']]\n",
    "assert len(combined_results['images']) == len(set(result_filenames))\n",
    "\n",
    "# im = combined_results['images'][0]\n",
    "for im in combined_results['images']:\n",
    "    assert im['file'].startswith(input_path + '/')\n",
    "    im['file']= im['file'].replace(input_path + '/','',1)\n",
    "\n",
    "combined_api_output_file = os.path.join(\n",
    "    combined_api_output_folder,\n",
    "    '{}_detections.json'.format(base_task_name))\n",
    "\n",
    "with open(combined_api_output_file,'w') as f:\n",
    "    json.dump(combined_results,f,indent=2)\n",
    "\n",
    "print('Wrote results to {}'.format(combined_api_output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0f262d",
   "metadata": {},
   "source": [
    "## Post-processing (no ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6238f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_animals_only = False\n",
    "\n",
    "options = PostProcessingOptions()\n",
    "options.image_base_dir = input_path\n",
    "options.parallelize_rendering = True\n",
    "options.include_almost_detections = True\n",
    "options.num_images_to_sample = 7500\n",
    "options.parallelize_rendering_n_cores = n_rendering_threads\n",
    "options.confidence_threshold = 0.2\n",
    "options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05\n",
    "options.ground_truth_json_file = None\n",
    "options.separate_detections_by_category = True\n",
    "# options.sample_seed = 0\n",
    "\n",
    "if render_animals_only:\n",
    "    # Omit some pages from the output, useful when animals are rare\n",
    "    options.rendering_bypass_sets = ['detections_person','detections_vehicle',\n",
    "                                     'detections_person_vehicle','non_detections']\n",
    "\n",
    "output_base = os.path.join(postprocessing_output_folder,\n",
    "    base_task_name + '_{:.3f}'.format(options.confidence_threshold))\n",
    "if render_animals_only:\n",
    "    output_base = output_base + '_animals_only'\n",
    "\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "print('Processing to {}'.format(output_base))\n",
    "\n",
    "options.api_output_file = combined_api_output_file\n",
    "options.output_dir = output_base\n",
    "ppresults = process_batch_results(options)\n",
    "html_output_file = ppresults.output_html_file\n",
    "path_utils.open_file(html_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33545eb2",
   "metadata": {},
   "source": [
    "## RDE (sample directory collapsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28deab49",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overflow_folders(relativePath):\n",
    "\n",
    "    import re\n",
    "\n",
    "    # In this example, the camera created folders called \"100EK113\", \"101EK113\", etc., for every N images\n",
    "    pat = '\\/\\d+EK\\d+\\/'\n",
    "\n",
    "    relativePath = relativePath.replace('\\\\','/')\n",
    "    relativePath = re.sub(pat,'/',relativePath)\n",
    "    dirName = os.path.dirname(relativePath)\n",
    "\n",
    "    return dirName\n",
    "\n",
    "if False:\n",
    "\n",
    "    pass\n",
    "\n",
    "    #%%\n",
    "\n",
    "    relativePath = 'a/b/c/d/100EK113/blah.jpg'\n",
    "    print(remove_overflow_folders(relativePath))\n",
    "\n",
    "    #%%\n",
    "\n",
    "    with open(combined_api_output_file,'r') as f:\n",
    "        d = json.load(f)\n",
    "    image_filenames = [im['file'] for im in d['images']]\n",
    "\n",
    "    # relativePath = image_filenames[0]\n",
    "    for relativePath in tqdm(image_filenames):\n",
    "        remove_overflow_folders(relativePath)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "157a72c7",
   "metadata": {},
   "source": [
    "## Repeat detection elimination, phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c66537d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliberately leaving these imports here, rather than at the top, because this\n",
    "# cell is not typically executed\n",
    "from api.batch_processing.postprocessing.repeat_detection_elimination import repeat_detections_core\n",
    "import path_utils\n",
    "task_index = 0\n",
    "\n",
    "options = repeat_detections_core.RepeatDetectionOptions()\n",
    "\n",
    "options.confidenceMin = 0.15\n",
    "options.confidenceMax = 1.01\n",
    "options.iouThreshold = 0.85\n",
    "options.occurrenceThreshold = 10\n",
    "options.maxSuspiciousDetectionSize = 0.2\n",
    "# options.minSuspiciousDetectionSize = 0.05\n",
    "\n",
    "# This will cause a very light gray box to get drawn around all the detections\n",
    "# we're *not* considering as suspicious.\n",
    "options.bRenderOtherDetections = True\n",
    "options.otherDetectionsThreshold = options.confidenceMin\n",
    "\n",
    "# options.lineThickness = 5\n",
    "# options.boxExpansion = 8\n",
    "\n",
    "# To invoke custom collapsing of folders for a particular manufacturer's naming scheme\n",
    "# options.customDirNameFunction = remove_overflow_folders\n",
    "\n",
    "options.bRenderHtml = False\n",
    "options.imageBase = input_path\n",
    "rde_string = 'rde_{:.2f}_{:.2f}_{}_{:.2f}'.format(\n",
    "    options.confidenceMin, options.iouThreshold,\n",
    "    options.occurrenceThreshold, options.maxSuspiciousDetectionSize)\n",
    "options.outputBase = os.path.join(filename_base, rde_string + '_task_{}'.format(task_index))\n",
    "options.filenameReplacements = {'':''}\n",
    "\n",
    "# Exclude people and vehicles from RDE\n",
    "# options.excludeClasses = [2,3]\n",
    "\n",
    "# options.maxImagesPerFolder = 50000\n",
    "# options.includeFolders = ['a/b/c']\n",
    "# options.excludeFolder = ['a/b/c']\n",
    "\n",
    "options.debugMaxDir = -1\n",
    "options.debugMaxRenderDir = -1\n",
    "options.debugMaxRenderDetection = -1\n",
    "options.debugMaxRenderInstance = -1\n",
    "\n",
    "# Can be None, 'xsort', or 'clustersort'\n",
    "options.smartSort = 'xsort'\n",
    "\n",
    "suspiciousDetectionResults = repeat_detections_core.find_repeat_detections(combined_api_output_file,\n",
    "                                                                           None,\n",
    "                                                                           options)\n",
    "\n",
    "# import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))\n",
    "# path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f2b9997",
   "metadata": {},
   "source": [
    "## Manual RDE step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "486239f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE THE VALID DETECTIONS ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab08d46",
   "metadata": {},
   "source": [
    "## Re-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b2368b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.batch_processing.postprocessing.repeat_detection_elimination import remove_repeat_detections\n",
    "\n",
    "filtered_output_filename = path_utils.insert_before_extension(combined_api_output_file, 'filtered_{}'.format(rde_string))\n",
    "\n",
    "remove_repeat_detections.remove_repeat_detections(\n",
    "    inputFile=combined_api_output_file,\n",
    "    outputFile=filtered_output_filename,\n",
    "    filteringDir=os.path.dirname(suspiciousDetectionResults.filterFile)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e4f6e4",
   "metadata": {},
   "source": [
    "## Post-processing (post-RDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c2d185",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_animals_only = False\n",
    "\n",
    "options = PostProcessingOptions()\n",
    "options.image_base_dir = input_path\n",
    "options.parallelize_rendering = True\n",
    "options.include_almost_detections = True\n",
    "options.num_images_to_sample = 7500\n",
    "options.confidence_threshold = 0.2\n",
    "options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05\n",
    "options.ground_truth_json_file = None\n",
    "options.separate_detections_by_category = True\n",
    "# options.sample_seed = 0\n",
    "\n",
    "if render_animals_only:\n",
    "    # Omit some pages from the output, useful when animals are rare\n",
    "    options.rendering_bypass_sets = ['detections_person','detections_vehicle',\n",
    "                                      'detections_person_vehicle','non_detections']\n",
    "\n",
    "output_base = os.path.join(postprocessing_output_folder,\n",
    "    base_task_name + '_{}_{:.3f}'.format(rde_string, options.confidence_threshold))\n",
    "\n",
    "if render_animals_only:\n",
    "    output_base = output_base + '_render_animals_only'\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "print('Processing post-RDE to {}'.format(output_base))\n",
    "\n",
    "options.api_output_file = filtered_output_filename\n",
    "options.output_dir = output_base\n",
    "ppresults = process_batch_results(options)\n",
    "html_output_file = ppresults.output_html_file\n",
    "\n",
    "path_utils.open_file(html_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b40a6ac",
   "metadata": {},
   "source": [
    "## Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42007804",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name_short = 'megaclassifier'\n",
    "threshold_str = '0.15' # 0.6\n",
    "classifier_name = 'megaclassifier_v0.1_efficientnet-b3'\n",
    "\n",
    "organization_name = organization_name_short\n",
    "job_name = base_task_name\n",
    "input_filename = filtered_output_filename # combined_api_output_file\n",
    "input_files = [input_filename]\n",
    "image_base = input_path\n",
    "crop_path = os.path.join(os.path.expanduser('~/crops'),job_name + '_crops')\n",
    "output_base = combined_api_output_folder\n",
    "device_id = 0\n",
    "\n",
    "output_file = os.path.join(filename_base,'run_{}_'.format(classifier_name_short) + job_name +  '.sh')\n",
    "\n",
    "classifier_base = os.path.expanduser('~/models/camera_traps/megaclassifier/v0.1/')\n",
    "assert os.path.isdir(classifier_base)\n",
    "\n",
    "checkpoint_path = os.path.join(classifier_base,'v0.1_efficientnet-b3_compiled.pt')\n",
    "assert os.path.isfile(checkpoint_path)\n",
    "\n",
    "classifier_categories_path = os.path.join(classifier_base,'v0.1_index_to_name.json')\n",
    "assert os.path.isfile(classifier_categories_path)\n",
    "\n",
    "target_mapping_path = os.path.join(classifier_base,'idfg_to_megaclassifier_labels.json')\n",
    "assert os.path.isfile(target_mapping_path)\n",
    "\n",
    "classifier_output_suffix = '_megaclassifier_output.csv.gz'\n",
    "final_output_suffix = '_megaclassifier.json'\n",
    "\n",
    "n_threads_str = '50'\n",
    "image_size_str = '300'\n",
    "batch_size_str = '64'\n",
    "num_workers_str = '8'\n",
    "classification_threshold_str = '0.05'\n",
    "\n",
    "logdir = filename_base\n",
    "\n",
    "# This is just passed along to the metadata in the output file, it has no impact\n",
    "# on how the classification scripts run.\n",
    "typical_classification_threshold_str = '0.75'\n",
    "\n",
    "##%% Set up environment\n",
    "\n",
    "commands = []\n",
    "# commands.append('cd CameraTraps/classification\\n')\n",
    "# commands.append('conda activate cameratraps-classifier\\n')\n",
    "\n",
    "\n",
    "##%% Crop images\n",
    "\n",
    "commands.append('\\n### Cropping ###\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    crop_cmd = ''\n",
    "\n",
    "    crop_comment = '\\n# Cropping {}\\n'.format(fn)\n",
    "    crop_cmd += crop_comment\n",
    "\n",
    "    crop_cmd += \"python crop_detections.py \\\\\\n\" + \\\n",
    "    \t input_file_path + ' \\\\\\n' + \\\n",
    "         crop_path + ' \\\\\\n' + \\\n",
    "         '--images-dir \"' + image_base + '\"' + ' \\\\\\n' + \\\n",
    "         '--threshold \"' + threshold_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--square-crops ' + ' \\\\\\n' + \\\n",
    "         '--threads \"' + n_threads_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--logdir \"' + logdir + '\"' + ' \\\\\\n' + \\\n",
    "         '\\n'\n",
    "    crop_cmd = '{}'.format(crop_cmd)\n",
    "    commands.append(crop_cmd)\n",
    "\n",
    "\n",
    "##%% Run classifier\n",
    "\n",
    "commands.append('\\n### Classifying ###\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "\n",
    "    classify_cmd = ''\n",
    "\n",
    "    classify_comment = '\\n# Classifying {}\\n'.format(fn)\n",
    "    classify_cmd += classify_comment\n",
    "\n",
    "    classify_cmd += \"python run_classifier.py \\\\\\n\" + \\\n",
    "    \t checkpoint_path + ' \\\\\\n' + \\\n",
    "         crop_path + ' \\\\\\n' + \\\n",
    "         classifier_output_path + ' \\\\\\n' + \\\n",
    "         '--detections-json \"' + input_file_path + '\"' + ' \\\\\\n' + \\\n",
    "         '--classifier-categories \"' + classifier_categories_path + '\"' + ' \\\\\\n' + \\\n",
    "         '--image-size \"' + image_size_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--batch-size \"' + batch_size_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--num-workers \"' + num_workers_str + '\"' + ' \\\\\\n'\n",
    "\n",
    "    if device_id is not None:\n",
    "        classify_cmd += '--device {}'.format(device_id)\n",
    "\n",
    "    classify_cmd += '\\n\\n'\n",
    "    classify_cmd = '{}'.format(classify_cmd)\n",
    "    commands.append(classify_cmd)\n",
    "\n",
    "\n",
    "##%% Remap classifier outputs\n",
    "\n",
    "commands.append('\\n### Remapping ###\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "    classifier_output_path_remapped = \\\n",
    "        classifier_output_path.replace(\".csv.gz\",\"_remapped.csv.gz\")\n",
    "    assert not (classifier_output_path == classifier_output_path_remapped)\n",
    "\n",
    "    output_label_index = classifier_output_path_remapped.replace(\n",
    "        \"_remapped.csv.gz\",\"_label_index_remapped.json\")\n",
    "\n",
    "    remap_cmd = ''\n",
    "\n",
    "    remap_comment = '\\n# Remapping {}\\n'.format(fn)\n",
    "    remap_cmd += remap_comment\n",
    "\n",
    "    remap_cmd += \"python aggregate_classifier_probs.py \\\\\\n\" + \\\n",
    "        classifier_output_path + ' \\\\\\n' + \\\n",
    "        '--target-mapping \"' + target_mapping_path + '\"' + ' \\\\\\n' + \\\n",
    "        '--output-csv \"' + classifier_output_path_remapped + '\"' + ' \\\\\\n' + \\\n",
    "        '--output-label-index \"' + output_label_index + '\"' + ' \\\\\\n' + \\\n",
    "        '\\n'\n",
    "\n",
    "    remap_cmd = '{}'.format(remap_cmd)\n",
    "    commands.append(remap_cmd)\n",
    "\n",
    "\n",
    "##%% Merge classification and detection outputs\n",
    "\n",
    "commands.append('\\n### Merging ###\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "\n",
    "    classifier_output_path_remapped = \\\n",
    "        classifier_output_path.replace(\".csv.gz\",\"_remapped.csv.gz\")\n",
    "\n",
    "    output_label_index = classifier_output_path_remapped.replace(\n",
    "        \"_remapped.csv.gz\",\"_label_index_remapped.json\")\n",
    "\n",
    "    final_output_path = os.path.join(output_base,\n",
    "                                     os.path.basename(classifier_output_path)).\\\n",
    "        replace(classifier_output_suffix,\n",
    "        final_output_suffix)\n",
    "    final_output_path = final_output_path.replace('_detections','')\n",
    "    final_output_path = final_output_path.replace('_crops','')\n",
    "    final_output_path_mc = final_output_path\n",
    "\n",
    "    merge_cmd = ''\n",
    "\n",
    "    merge_comment = '\\n# Merging {}\\n'.format(fn)\n",
    "    merge_cmd += merge_comment\n",
    "\n",
    "    merge_cmd += \"python merge_classification_detection_output.py \\\\\\n\" + \\\n",
    "    \t classifier_output_path_remapped + ' \\\\\\n' + \\\n",
    "         output_label_index + ' \\\\\\n' + \\\n",
    "         '--output-json \"' + final_output_path + '\"' + ' \\\\\\n' + \\\n",
    "         '--detection-json \"' + input_file_path + '\"' + ' \\\\\\n' + \\\n",
    "         '--classifier-name \"' + classifier_name + '\"' + ' \\\\\\n' + \\\n",
    "         '--threshold \"' + classification_threshold_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--typical-confidence-threshold \"' + typical_classification_threshold_str + '\"' + ' \\\\\\n' + \\\n",
    "         '\\n'\n",
    "    merge_cmd = '{}'.format(merge_cmd)\n",
    "    commands.append(merge_cmd)\n",
    "\n",
    "\n",
    "##%% Write  out classification script\n",
    "\n",
    "with open(output_file,'w') as f:\n",
    "    for s in commands:\n",
    "        f.write('{}'.format(s))\n",
    "\n",
    "import stat\n",
    "st = os.stat(output_file)\n",
    "os.chmod(output_file, st.st_mode | stat.S_IEXEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb99ee3a",
   "metadata": {},
   "source": [
    "## Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94505958",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name_short = 'idfgclassifier'\n",
    "threshold_str = '0.15' # 0.6\n",
    "classifier_name = 'idfg_classifier_ckpt_14_compiled'\n",
    "\n",
    "organization_name = organization_name_short\n",
    "job_name = base_task_name\n",
    "input_filename = filtered_output_filename # combined_api_output_file\n",
    "input_files = [input_filename]\n",
    "image_base = input_path\n",
    "crop_path = os.path.join(os.path.expanduser('~/crops'),job_name + '_crops')\n",
    "output_base = combined_api_output_folder\n",
    "device_id = 1\n",
    "\n",
    "output_file = os.path.join(filename_base,'run_{}_'.format(classifier_name_short) + job_name +  '.sh')\n",
    "\n",
    "classifier_base = os.path.expanduser('~/models/camera_traps/idfg_classifier/idfg_classifier_20200905_042558')\n",
    "assert os.path.isdir(classifier_base)\n",
    "\n",
    "checkpoint_path = os.path.join(classifier_base,'idfg_classifier_ckpt_14_compiled.pt')\n",
    "assert os.path.isfile(checkpoint_path)\n",
    "\n",
    "classifier_categories_path = os.path.join(classifier_base,'label_index.json')\n",
    "assert os.path.isfile(classifier_categories_path)\n",
    "\n",
    "classifier_output_suffix = '_{}_output.csv.gz'.format(classifier_name_short)\n",
    "final_output_suffix = '_{}.json'.format(classifier_name_short)\n",
    "\n",
    "threshold_str = '0.65'\n",
    "n_threads_str = '50'\n",
    "image_size_str = '300'\n",
    "batch_size_str = '64'\n",
    "num_workers_str = '8'\n",
    "logdir = filename_base\n",
    "\n",
    "classification_threshold_str = '0.05'\n",
    "\n",
    "# This is just passed along to the metadata in the output file, it has no impact\n",
    "# on how the classification scripts run.\n",
    "typical_classification_threshold_str = '0.75'\n",
    "\n",
    "\n",
    "##%% Set up environment\n",
    "\n",
    "commands = []\n",
    "\n",
    "\n",
    "##%% Crop images\n",
    "\n",
    "commands.append('\\n### Cropping ###\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    crop_cmd = ''\n",
    "\n",
    "    crop_comment = '\\n# Cropping {}\\n'.format(fn)\n",
    "    crop_cmd += crop_comment\n",
    "\n",
    "    crop_cmd += \"python crop_detections.py \\\\\\n\" + \\\n",
    "    \t input_file_path + ' \\\\\\n' + \\\n",
    "         crop_path + ' \\\\\\n' + \\\n",
    "         '--images-dir \"' + image_base + '\"' + ' \\\\\\n' + \\\n",
    "         '--threshold \"' + threshold_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--square-crops ' + ' \\\\\\n' + \\\n",
    "         '--threads \"' + n_threads_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--logdir \"' + logdir + '\"' + ' \\\\\\n' + \\\n",
    "         '\\n'\n",
    "    crop_cmd = '{}'.format(crop_cmd)\n",
    "    commands.append(crop_cmd)\n",
    "\n",
    "\n",
    "##%% Run classifier\n",
    "\n",
    "commands.append('\\n### Classifying ###\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "\n",
    "    classify_cmd = ''\n",
    "\n",
    "    classify_comment = '\\n# Classifying {}\\n'.format(fn)\n",
    "    classify_cmd += classify_comment\n",
    "\n",
    "    classify_cmd += \"python run_classifier.py \\\\\\n\" + \\\n",
    "    \t checkpoint_path + ' \\\\\\n' + \\\n",
    "         crop_path + ' \\\\\\n' + \\\n",
    "         classifier_output_path + ' \\\\\\n' + \\\n",
    "         '--detections-json \"' + input_file_path + '\"' + ' \\\\\\n' + \\\n",
    "         '--classifier-categories \"' + classifier_categories_path + '\"' + ' \\\\\\n' + \\\n",
    "         '--image-size \"' + image_size_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--batch-size \"' + batch_size_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--num-workers \"' + num_workers_str + '\"' + ' \\\\\\n'\n",
    "\n",
    "    if device_id is not None:\n",
    "        classify_cmd += '--device {}'.format(device_id)\n",
    "\n",
    "    classify_cmd += '\\n\\n'\n",
    "    classify_cmd = '{}'.format(classify_cmd)\n",
    "    commands.append(classify_cmd)\n",
    "\n",
    "\n",
    "##%% Merge classification and detection outputs\n",
    "\n",
    "commands.append('\\n### Merging ###\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "    final_output_path = os.path.join(output_base,\n",
    "                                     os.path.basename(classifier_output_path)).\\\n",
    "                                     replace(classifier_output_suffix,\n",
    "                                     final_output_suffix)\n",
    "    final_output_path = final_output_path.replace('_detections','')\n",
    "    final_output_path = final_output_path.replace('_crops','')\n",
    "    final_output_path_ic = final_output_path\n",
    "\n",
    "    merge_cmd = ''\n",
    "\n",
    "    merge_comment = '\\n# Merging {}\\n'.format(fn)\n",
    "    merge_cmd += merge_comment\n",
    "\n",
    "    merge_cmd += \"python merge_classification_detection_output.py \\\\\\n\" + \\\n",
    "    \t classifier_output_path + ' \\\\\\n' + \\\n",
    "         classifier_categories_path + ' \\\\\\n' + \\\n",
    "         '--output-json \"' + final_output_path_ic + '\"' + ' \\\\\\n' + \\\n",
    "         '--detection-json \"' + input_file_path + '\"' + ' \\\\\\n' + \\\n",
    "         '--classifier-name \"' + classifier_name + '\"' + ' \\\\\\n' + \\\n",
    "         '--threshold \"' + classification_threshold_str + '\"' + ' \\\\\\n' + \\\n",
    "         '--typical-confidence-threshold \"' + typical_classification_threshold_str + '\"' + ' \\\\\\n' + \\\n",
    "         '\\n'\n",
    "    merge_cmd = '{}'.format(merge_cmd)\n",
    "    commands.append(merge_cmd)\n",
    "\n",
    "\n",
    "##%% Write everything out\n",
    "\n",
    "with open(output_file,'w') as f:\n",
    "    for s in commands:\n",
    "        f.write('{}'.format(s))\n",
    "\n",
    "import stat\n",
    "st = os.stat(output_file)\n",
    "os.chmod(output_file, st.st_mode | stat.S_IEXEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51689148",
   "metadata": {},
   "source": [
    "## Within-image classification smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eda357d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only count detections with a classification confidence threshold above\n",
    "# *classification_confidence_threshold*, which in practice means we're only\n",
    "# looking at one category per detection.\n",
    "#\n",
    "# If an image has at least *min_detections_above_threshold* such detections\n",
    "# in the most common category, and no more than *max_detections_secondary_class*\n",
    "# in the second-most-common category, flip all detections to the most common\n",
    "# category.\n",
    "#\n",
    "# Optionally treat some classes as particularly unreliable, typically used to overwrite an\n",
    "# \"other\" class.\n",
    "\n",
    "classification_detection_files = [\n",
    "    final_output_path_mc,\n",
    "    final_output_path_ic\n",
    "    ]\n",
    "\n",
    "assert all([os.path.isfile(fn) for fn in classification_detection_files])\n",
    "\n",
    "# Only count detections with a classification confidence threshold above\n",
    "# *classification_confidence_threshold*, which in practice means we're only\n",
    "# looking at one category per detection.\n",
    "#\n",
    "# If an image has at least *min_detections_above_threshold* such detections\n",
    "# in the most common category, and no more than *max_detections_secondary_class*\n",
    "# in the second-most-common category, flip all detections to the most common\n",
    "# category.\n",
    "#\n",
    "# Optionally treat some classes as particularly unreliable, typically used to overwrite an\n",
    "# \"other\" class.\n",
    "\n",
    "smoothed_classification_files = []\n",
    "\n",
    "for final_output_path in classification_detection_files:\n",
    "\n",
    "    classifier_output_path = final_output_path\n",
    "    classifier_output_path_within_image_smoothing = classifier_output_path.replace(\n",
    "        '.json','_within_image_smoothing.json')\n",
    "\n",
    "    with open(classifier_output_path,'r') as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    # d['classification_categories']\n",
    "\n",
    "    # im['detections']\n",
    "\n",
    "    # path_utils.open_file(os.path.join(input_path,im['file']))\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    min_detections_above_threshold = 4\n",
    "    max_detections_secondary_class = 3\n",
    "\n",
    "    min_detections_to_overwrite_other = 2\n",
    "    other_category_names = ['other']\n",
    "\n",
    "    classification_confidence_threshold = 0.6\n",
    "\n",
    "    # Which classifications should we even bother over-writing?\n",
    "    classification_overwrite_threshold = 0.3 # classification_confidence_threshold\n",
    "\n",
    "    # Detection confidence threshold for things we count\n",
    "    detection_confidence_threshold = 0.2\n",
    "\n",
    "    # Which detections should we even bother over-writing?\n",
    "    detection_overwrite_threshold = 0.05\n",
    "\n",
    "    category_name_to_id = {d['classification_categories'][k]:k for k in d['classification_categories']}\n",
    "    other_category_ids = []\n",
    "    for s in other_category_names:\n",
    "        if s in category_name_to_id:\n",
    "            other_category_ids.append(category_name_to_id[s])\n",
    "        else:\n",
    "            print('Warning: \"other\" category {} not present in file {}'.format(\n",
    "                s,classifier_output_path))\n",
    "\n",
    "    n_other_classifications_changed = 0\n",
    "    n_other_images_changed = 0\n",
    "\n",
    "    n_detections_flipped = 0\n",
    "    n_images_changed = 0\n",
    "\n",
    "    # im = d['images'][0]\n",
    "    for im in tqdm(d['images']):\n",
    "\n",
    "        if 'detections' not in im or im['detections'] is None or len(im['detections']) == 0:\n",
    "            continue\n",
    "\n",
    "        detections = im['detections']\n",
    "\n",
    "        category_to_count = defaultdict(int)\n",
    "        for det in detections:\n",
    "            if ('classifications' in det) and (det['conf'] >= detection_confidence_threshold):\n",
    "                for c in det['classifications']:\n",
    "                    if c[1] >= classification_confidence_threshold:\n",
    "                        category_to_count[c[0]] += 1\n",
    "                # ...for each classification\n",
    "            # ...if there are classifications for this detection\n",
    "        # ...for each detection\n",
    "\n",
    "        if len(category_to_count) <= 1:\n",
    "            continue\n",
    "\n",
    "        category_to_count = {k: v for k, v in sorted(category_to_count.items(),\n",
    "                                                     key=lambda item: item[1],\n",
    "                                                     reverse=True)}\n",
    "\n",
    "        keys = list(category_to_count.keys())\n",
    "\n",
    "        # Handle a quirky special case: if the most common category is \"other\" and\n",
    "        # it's \"tied\" with the second-most-common category, swap them\n",
    "        if (len(keys) > 1) and (keys[0] in other_category_ids) and (keys[1] not in other_category_ids) and\\\n",
    "            (category_to_count[keys[0]] == category_to_count[keys[1]]):\n",
    "                keys[1], keys[0] = keys[0], keys[1]\n",
    "\n",
    "        max_count = category_to_count[keys[0]]\n",
    "        # secondary_count = category_to_count[keys[1]]\n",
    "        # The 'secondary count' is the most common non-other class\n",
    "        secondary_count = 0\n",
    "        for i_key in range(1,len(keys)):\n",
    "            if keys[i_key] not in other_category_ids:\n",
    "                secondary_count = category_to_count[keys[i_key]]\n",
    "                break\n",
    "\n",
    "        most_common_category = keys[0]\n",
    "\n",
    "        assert max_count >= secondary_count\n",
    "\n",
    "        # If we have at least *min_detections_to_overwrite_other* in a category that isn't\n",
    "        # \"other\", change all \"other\" classifications to that category\n",
    "        if max_count >= min_detections_to_overwrite_other and \\\n",
    "            most_common_category not in other_category_ids:\n",
    "\n",
    "            other_change_made = False\n",
    "\n",
    "            for det in detections:\n",
    "\n",
    "                if ('classifications' in det) and (det['conf'] >= detection_overwrite_threshold):\n",
    "\n",
    "                    for c in det['classifications']:\n",
    "\n",
    "                        if c[1] >= classification_overwrite_threshold and \\\n",
    "                            c[0] in other_category_ids:\n",
    "\n",
    "                            n_other_classifications_changed += 1\n",
    "                            other_change_made = True\n",
    "                            c[0] = most_common_category\n",
    "\n",
    "                    # ...for each classification\n",
    "\n",
    "                # ...if there are classifications for this detection\n",
    "\n",
    "            # ...for each detection\n",
    "\n",
    "            if other_change_made:\n",
    "                n_other_images_changed += 1\n",
    "\n",
    "        # ...if we should overwrite all \"other\" classifications\n",
    "\n",
    "        if max_count < min_detections_above_threshold:\n",
    "            continue\n",
    "\n",
    "        if secondary_count >= max_detections_secondary_class:\n",
    "            continue\n",
    "\n",
    "        # At this point, we know we have a dominant category; change all other above-threshold\n",
    "        # classifications to that category.  That category may have been \"other\", in which\n",
    "        # case we may have already made the relevant changes.\n",
    "\n",
    "        n_detections_flipped_this_image = 0\n",
    "\n",
    "        # det = detections[0]\n",
    "        for det in detections:\n",
    "\n",
    "            if ('classifications' in det) and (det['conf'] >= detection_overwrite_threshold):\n",
    "\n",
    "                for c in det['classifications']:\n",
    "                    if c[1] >= classification_overwrite_threshold and \\\n",
    "                        c[0] != most_common_category:\n",
    "\n",
    "                        c[0] = most_common_category\n",
    "                        n_detections_flipped += 1\n",
    "                        n_detections_flipped_this_image += 1\n",
    "\n",
    "                # ...for each classification\n",
    "\n",
    "            # ...if there are classifications for this detection\n",
    "\n",
    "        # ...for each detection\n",
    "\n",
    "        if n_detections_flipped_this_image > 0:\n",
    "            n_images_changed += 1\n",
    "\n",
    "    # ...for each image\n",
    "\n",
    "    print('Classification smoothing: changed {} detections on {} images'.format(\n",
    "        n_detections_flipped,n_images_changed))\n",
    "\n",
    "    print('\"Other\" smoothing: changed {} detections on {} images'.format(\n",
    "          n_other_classifications_changed,n_other_images_changed))\n",
    "\n",
    "    with open(classifier_output_path_within_image_smoothing,'w') as f:\n",
    "        json.dump(d,f,indent=2)\n",
    "\n",
    "    print('Wrote results to:\\n{}'.format(classifier_output_path_within_image_smoothing))\n",
    "    smoothed_classification_files.append(classifier_output_path_within_image_smoothing)\n",
    "\n",
    "# ...for each file we want to smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4325c5c1",
   "metadata": {},
   "source": [
    "## Post-processing (post-classification)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16c3a868",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_detection_files = smoothed_classification_files\n",
    "\n",
    "assert all([os.path.isfile(fn) for fn in classification_detection_files])\n",
    "\n",
    "# classification_detection_file = classification_detection_files[1]\n",
    "for classification_detection_file in classification_detection_files:\n",
    "\n",
    "    options = PostProcessingOptions()\n",
    "    options.image_base_dir = input_path\n",
    "    options.parallelize_rendering = True\n",
    "    options.include_almost_detections = True\n",
    "    options.num_images_to_sample = 10000\n",
    "    options.confidence_threshold = 0.2\n",
    "    options.classification_confidence_threshold = 0.75\n",
    "    options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05\n",
    "    options.ground_truth_json_file = None\n",
    "    options.separate_detections_by_category = True\n",
    "\n",
    "    folder_token = classification_detection_file.split('/')[-1].replace('classifier.json','')\n",
    "\n",
    "    output_base = os.path.join(postprocessing_output_folder, folder_token + \\\n",
    "        base_task_name + '_{:.3f}'.format(options.confidence_threshold))\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    print('Processing {} to {}'.format(base_task_name, output_base))\n",
    "\n",
    "    options.api_output_file = classification_detection_file\n",
    "    options.output_dir = output_base\n",
    "    ppresults = process_batch_results(options)\n",
    "    path_utils.open_file(ppresults.output_html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "403a5f18",
   "metadata": {},
   "source": [
    "## 99.9% of jobs end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b59ba81b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything after this is run ad hoc and requires some manual editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e7954e",
   "metadata": {},
   "source": [
    "## Compare results files for different model versions (or before/after RDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5fff757",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from api.batch_processing.postprocessing.compare_batch_results import (\n",
    "    BatchComparisonOptions,PairwiseBatchComparisonOptions,compare_batch_results)\n",
    "\n",
    "options = BatchComparisonOptions()\n",
    "\n",
    "options.job_name = organization_name_short\n",
    "options.output_folder = os.path.join(postprocessing_output_folder,'model_comparison')\n",
    "options.image_folder = input_path\n",
    "\n",
    "options.pairwise_options = []\n",
    "\n",
    "filenames = [\n",
    "    '/postprocessing/organization/mdv4_results.json',\n",
    "    '/postprocessing/organization/mdv5a_results.json',\n",
    "    '/postprocessing/organization/mdv5b_results.json'\n",
    "    ]\n",
    "\n",
    "detection_thresholds = [0.7,0.15,0.15]\n",
    "\n",
    "assert len(detection_thresholds) == len(filenames)\n",
    "\n",
    "rendering_thresholds = [(x*0.6666) for x in detection_thresholds]\n",
    "\n",
    "# Choose all pairwise combinations of the files in [filenames]\n",
    "for i, j in itertools.combinations(list(range(0,len(filenames))),2):\n",
    "\n",
    "    pairwise_options = PairwiseBatchComparisonOptions()\n",
    "\n",
    "    pairwise_options.results_filename_a = filenames[i]\n",
    "    pairwise_options.results_filename_b = filenames[j]\n",
    "\n",
    "    pairwise_options.rendering_confidence_threshold_a = rendering_thresholds[i]\n",
    "    pairwise_options.rendering_confidence_threshold_b = rendering_thresholds[j]\n",
    "\n",
    "    pairwise_options.detection_thresholds_a = {'animal':detection_thresholds[i],\n",
    "                                               'person':detection_thresholds[i],\n",
    "                                               'vehicle':detection_thresholds[i]}\n",
    "    pairwise_options.detection_thresholds_b = {'animal':detection_thresholds[j],\n",
    "                                               'person':detection_thresholds[j],\n",
    "                                               'vehicle':detection_thresholds[j]}\n",
    "    options.pairwise_options.append(pairwise_options)\n",
    "\n",
    "results = compare_batch_results(options)\n",
    "\n",
    "from path_utils import open_file # from ai4eutils\n",
    "open_file(results.html_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aee96e0",
   "metadata": {},
   "source": [
    "## Merge in high-confidence detections from another results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b874b8d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.batch_processing.postprocessing.merge_detections import MergeDetectionsOptions,merge_detections\n",
    "\n",
    "source_files = ['']\n",
    "target_file = ''\n",
    "output_file = target_file.replace('.json','_merged.json')\n",
    "\n",
    "options = MergeDetectionsOptions()\n",
    "options.max_detection_size = 1.0\n",
    "options.target_confidence_threshold = 0.25\n",
    "options.categories_to_include = [1]\n",
    "options.source_confidence_thresholds = [0.2]\n",
    "merge_detections(source_files, target_file, output_file, options)\n",
    "\n",
    "merged_detections_file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df5d0333",
   "metadata": {},
   "source": [
    "## Create a new category for large boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51fc902",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.batch_processing.postprocessing import categorize_detections_by_size\n",
    "\n",
    "options = categorize_detections_by_size.SizeCategorizationOptions()\n",
    "\n",
    "# This is a size threshold, not a confidence threshold\n",
    "options.threshold = 0.85\n",
    "\n",
    "input_file = r\"g:\\organization\\file.json\"\n",
    "size_separated_file = input_file.replace('.json','-size-separated-{}.json'.format(options.threshold))\n",
    "d = categorize_detections_by_size.categorize_detections_by_size(input_file,size_separated_file,options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0616df6",
   "metadata": {},
   "source": [
    "## .json splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f363d06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "\n",
    "from api.batch_processing.postprocessing.subset_json_detector_output import (\n",
    "    subset_json_detector_output, SubsetJsonDetectorOutputOptions)\n",
    "\n",
    "input_filename = filtered_output_filename\n",
    "output_base = os.path.join(filename_base,'json_subsets')\n",
    "\n",
    "if False:\n",
    "    if data is None:\n",
    "        with open(input_filename) as f:\n",
    "            data = json.load(f)\n",
    "    print('Data set contains {} images'.format(len(data['images'])))\n",
    "\n",
    "print('Processing file {} to {}'.format(input_filename,output_base))\n",
    "\n",
    "options = SubsetJsonDetectorOutputOptions()\n",
    "# options.query = None\n",
    "# options.replacement = None\n",
    "\n",
    "options.split_folders = True\n",
    "options.make_folder_relative = True\n",
    "\n",
    "# Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom'\n",
    "options.split_folder_mode = 'bottom'  # 'top', 'n_from_top', 'n_from_bottom'\n",
    "options.split_folder_param = 0\n",
    "options.overwrite_json_files = False\n",
    "options.confidence_threshold = 0.01\n",
    "\n",
    "subset_data = subset_json_detector_output(input_filename, output_base, options, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb14ddb",
   "metadata": {},
   "source": [
    "## Custom splitting/subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a7315a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "\n",
    "from api.batch_processing.postprocessing.subset_json_detector_output import (\n",
    "    subset_json_detector_output, SubsetJsonDetectorOutputOptions)\n",
    "\n",
    "input_filename = filtered_output_filename\n",
    "output_base = os.path.join(filename_base,'json_subsets')\n",
    "\n",
    "folders = os.listdir(input_path)\n",
    "\n",
    "if data is None:\n",
    "    with open(input_filename) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "print('Data set contains {} images'.format(len(data['images'])))\n",
    "\n",
    "# i_folder = 0; folder_name = folders[i_folder]\n",
    "for i_folder, folder_name in enumerate(folders):\n",
    "\n",
    "    output_filename = os.path.join(output_base, folder_name + '.json')\n",
    "    print('Processing folder {} of {} ({}) to {}'.format(i_folder, len(folders), folder_name,\n",
    "          output_filename))\n",
    "\n",
    "    options = SubsetJsonDetectorOutputOptions()\n",
    "    options.confidence_threshold = 0.01\n",
    "    options.overwrite_json_files = True\n",
    "    options.query = folder_name + '/'\n",
    "\n",
    "    # This doesn't do anything in this case, since we're not splitting folders\n",
    "    # options.make_folder_relative = True\n",
    "\n",
    "    subset_data = subset_json_detector_output(input_filename, output_filename, options, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14835264",
   "metadata": {},
   "source": [
    "## String replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cc6165",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "\n",
    "from api.batch_processing.postprocessing.subset_json_detector_output import (\n",
    "    subset_json_detector_output, SubsetJsonDetectorOutputOptions)\n",
    "\n",
    "input_filename = filtered_output_filename\n",
    "output_filename = input_filename.replace('.json','_replaced.json')\n",
    "\n",
    "options = SubsetJsonDetectorOutputOptions()\n",
    "options.query = folder_name + '/'\n",
    "options.replacement = ''\n",
    "subset_json_detector_output(input_filename,output_filename,options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83363171",
   "metadata": {},
   "source": [
    "## Splitting images into folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debe7d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.batch_processing.postprocessing.separate_detections_into_folders import (\n",
    "    separate_detections_into_folders, SeparateDetectionsIntoFoldersOptions)\n",
    "\n",
    "default_threshold = 0.2\n",
    "base_output_folder = os.path.expanduser('~/data/{}-{}-separated'.format(base_task_name,default_threshold))\n",
    "\n",
    "options = SeparateDetectionsIntoFoldersOptions(default_threshold)\n",
    "\n",
    "options.results_file = filtered_output_filename\n",
    "options.base_input_folder = input_path\n",
    "options.base_output_folder = os.path.join(base_output_folder,folder_name)\n",
    "options.n_threads = 100\n",
    "options.allow_existing_directory = False\n",
    "\n",
    "separate_detections_into_folders(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cddb1a3",
   "metadata": {},
   "source": [
    "## Generate commands for a subset of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327c21c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = [8,10,12,14,16]; gpu_number = 0; sleep_time_between_tasks = 60; sleep_time_before_tasks = 0\n",
    "commands = []\n",
    "\n",
    "# i_task = 8\n",
    "for i_task in task_set:\n",
    "\n",
    "    if i_task == task_set[0]:\n",
    "        commands.append('sleep {}'.format(str(sleep_time_before_tasks)))\n",
    "\n",
    "    task = task_info[i_task]\n",
    "    chunk_file = task['input_file']\n",
    "    output_fn = chunk_file.replace('.json','_results.json')\n",
    "\n",
    "    task['output_file'] = output_fn\n",
    "\n",
    "    cuda_string = f'CUDA_VISIBLE_DEVICES={gpu_number}'\n",
    "\n",
    "    checkpoint_frequency_string = ''\n",
    "    checkpoint_path_string = ''\n",
    "    if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "        checkpoint_frequency_string = f'--checkpoint_frequency {checkpoint_frequency}'\n",
    "        checkpoint_path_string = '--checkpoint_path {}'.format(chunk_file.replace(\n",
    "            '.json','_checkpoint.json'))\n",
    "\n",
    "    use_image_queue_string = ''\n",
    "    if (use_image_queue):\n",
    "        use_image_queue_string = '--use_image_queue'\n",
    "\n",
    "    ncores_string = ''\n",
    "    if (ncores > 1):\n",
    "        ncores_string = '--ncores {}'.format(ncores)\n",
    "\n",
    "    quiet_string = ''\n",
    "    if quiet_mode:\n",
    "        quiet_string = '--quiet'\n",
    "\n",
    "    cmd = f'{cuda_string} python run_detector_batch.py {model_file} {chunk_file} {output_fn} {checkpoint_frequency_string} {checkpoint_path_string} {use_image_queue_string} {ncores_string} {quiet_string}'\n",
    "\n",
    "    task['command'] = cmd\n",
    "    commands.append(cmd)\n",
    "    if i_task != task_set[-1]:\n",
    "        commands.append('sleep {}'.format(str(sleep_time_between_tasks)))\n",
    "\n",
    "# ...for each task\n",
    "\n",
    "task_strings = [str(k).zfill(2) for k in task_set]\n",
    "task_set_string = '_'.join(task_strings)\n",
    "cmd_file = os.path.join(filename_base,'run_chunk_{}_gpu_{}.sh'.format(task_set_string,\n",
    "                        str(gpu_number).zfill(2)))\n",
    "\n",
    "with open(cmd_file,'w') as f:\n",
    "    for cmd in commands:\n",
    "        f.write(cmd + '\\n')\n",
    "\n",
    "import stat\n",
    "st = os.stat(cmd_file)\n",
    "os.chmod(cmd_file, st.st_mode | stat.S_IEXEC)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
