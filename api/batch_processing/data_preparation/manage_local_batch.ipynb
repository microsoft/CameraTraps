{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55c86b0b",
   "metadata": {},
   "source": [
    "# Managing a local MegaDetector batch\n",
    "\n",
    "This notebook represents an interactive process for running MegaDetector on large batches of images, including typical and optional postprocessing steps.  Everything after \"Merge results...\" is basically optional, and we typically do a mix of these optional steps, depending on the job.\n",
    "\n",
    "This notebook is auto-generated from manage_local_batch.py (a cell-delimited .py file that is used the same way, typically in Spyder or VS Code).    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e2064e6",
   "metadata": {},
   "source": [
    "## Imports and constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7d496f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import stat\n",
    "import time\n",
    "\n",
    "import humanfriendly\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from ai4eutils\n",
    "import ai4e_azure_utils\n",
    "import path_utils\n",
    "from ct_utils import is_list_sorted\n",
    "\n",
    "from detection.run_detector_batch import load_and_run_detector_batch, write_results_to_file\n",
    "from detection.run_detector import DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD\n",
    "\n",
    "from api.batch_processing.postprocessing.postprocess_batch_results import (\n",
    "    PostProcessingOptions, process_batch_results)\n",
    "from detection.run_detector import get_detector_version_from_filename\n",
    "\n",
    "max_task_name_length = 92\n",
    "\n",
    "# To specify a non-default confidence threshold for including detections in the .json file\n",
    "json_threshold = None\n",
    "\n",
    "# Turn warnings into errors if more than this many images are missing\n",
    "max_tolerable_failed_images = 100\n",
    "\n",
    "use_image_queue = False\n",
    "\n",
    "# Only relevant when we're using a single GPU\n",
    "default_gpu_number = 0\n",
    "\n",
    "quiet_mode = True\n",
    "\n",
    "# Specify a target image size when running MD... strongly recommended to leave this at \"None\"\n",
    "image_size = None\n",
    "\n",
    "# Only relevant when running on CPU\n",
    "ncores = 1\n",
    "\n",
    "# OS-specific script line continuation character\n",
    "slcc = '\\\\'\n",
    "\n",
    "# OS-specific script comment character\n",
    "scc = '#'\n",
    "\n",
    "script_extension = '.sh'\n",
    "\n",
    "# Prefer threads on Windows, processes on Linux\n",
    "parallelization_defaults_to_threads = False\n",
    "\n",
    "# This is for things like image rendering, not for MegaDetector\n",
    "default_workers_for_parallel_tasks = 30\n",
    "\n",
    "if os.name == 'nt':\n",
    "    slcc = '^'\n",
    "    scc = 'REM'\n",
    "    script_extension = '.bat'\n",
    "    parallelization_defaults_to_threads = True\n",
    "    default_workers_for_parallel_tasks = 10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815af58f",
   "metadata": {},
   "source": [
    "## Constants I set per script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67646b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_path = '/datadrive/organization/data'\n",
    "\n",
    "organization_name_short = 'organization'\n",
    "job_date = None # '2023-04-00'\n",
    "assert job_date is not None and organization_name_short != 'organization'\n",
    "\n",
    "# Optional descriptor\n",
    "job_tag = None\n",
    "\n",
    "if job_tag is None:\n",
    "    job_description_string = ''\n",
    "else:\n",
    "    job_description_string = '-' + job_tag\n",
    "\n",
    "model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5a.0.0.pt')\n",
    "# model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v5.0.0/md_v5b.0.0.pt')\n",
    "# model_file = os.path.expanduser('~/models/camera_traps/megadetector/md_v4.1.0/md_v4.1.0.pb')\n",
    "\n",
    "postprocessing_base = os.path.expanduser('~/postprocessing')\n",
    "\n",
    "# Number of jobs to split data into, typically equal to the number of available GPUs\n",
    "n_jobs = 2\n",
    "n_gpus = 2\n",
    "\n",
    "# Only used to print out a time estimate\n",
    "if ('v5') in model_file:\n",
    "    gpu_images_per_second = 10\n",
    "else:\n",
    "    gpu_images_per_second = 2.9\n",
    "\n",
    "checkpoint_frequency = 10000\n",
    "\n",
    "base_task_name = organization_name_short + '-' + job_date + job_description_string + '-' + get_detector_version_from_filename(model_file)\n",
    "base_output_folder_name = os.path.join(postprocessing_base,organization_name_short)\n",
    "os.makedirs(base_output_folder_name,exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ffa47b3",
   "metadata": {},
   "source": [
    "## Derived variables, path setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006a8b8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename_base = os.path.join(base_output_folder_name, base_task_name)\n",
    "combined_api_output_folder = os.path.join(filename_base, 'combined_api_outputs')\n",
    "postprocessing_output_folder = os.path.join(filename_base, 'preview')\n",
    "\n",
    "os.makedirs(filename_base, exist_ok=True)\n",
    "os.makedirs(combined_api_output_folder, exist_ok=True)\n",
    "os.makedirs(postprocessing_output_folder, exist_ok=True)\n",
    "\n",
    "if input_path.endswith('/'):\n",
    "    input_path = input_path[0:-1]\n",
    "\n",
    "print('Output folder:\\n{}'.format(filename_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e8dfc0c",
   "metadata": {},
   "source": [
    "## Enumerate files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a991b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_images = path_utils.find_images(input_path,recursive=True)\n",
    "\n",
    "print('Enumerated {} image files in {}'.format(len(all_images),input_path))\n",
    "\n",
    "if False:\n",
    "\n",
    "    pass\n",
    "\n",
    "    #%% Load files from prior enumeration\n",
    "\n",
    "    import re\n",
    "    chunk_files = os.listdir(filename_base)\n",
    "    pattern = re.compile('chunk\\d+.json')\n",
    "    chunk_files = [fn for fn in chunk_files if pattern.match(fn)]\n",
    "    all_images = []\n",
    "    for fn in chunk_files:\n",
    "        with open(os.path.join(filename_base,fn),'r') as f:\n",
    "            chunk = json.load(f)\n",
    "            assert isinstance(chunk,list)\n",
    "            all_images.extend(chunk)\n",
    "    print('Loaded {} image files from chunks in {}'.format(len(all_images),filename_base))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e2e67f",
   "metadata": {},
   "source": [
    "## Divide images into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038530b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_list(L, n):\n",
    "    k, m = divmod(len(L), n)\n",
    "    return list(L[i*k+min(i, m):(i+1)*k+min(i+1, m)] for i in range(n))\n",
    "\n",
    "folder_chunks = split_list(all_images,n_jobs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10210062",
   "metadata": {},
   "source": [
    "## Estimate total time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "327a55a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_images = len(all_images)\n",
    "execution_seconds = n_images / gpu_images_per_second\n",
    "wallclock_seconds = execution_seconds / n_gpus\n",
    "print('Expected time: {}'.format(humanfriendly.format_timespan(wallclock_seconds)))\n",
    "\n",
    "seconds_per_chunk = len(folder_chunks[0]) / gpu_images_per_second\n",
    "print('Expected time per chunk: {}'.format(humanfriendly.format_timespan(seconds_per_chunk)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cb34d9",
   "metadata": {},
   "source": [
    "## Write file lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "499bd31e",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_info = []\n",
    "\n",
    "for i_chunk,chunk_list in enumerate(folder_chunks):\n",
    "\n",
    "    chunk_fn = os.path.join(filename_base,'chunk{}.json'.format(str(i_chunk).zfill(3)))\n",
    "    task_info.append({'id':i_chunk,'input_file':chunk_fn})\n",
    "    ai4e_azure_utils.write_list_to_file(chunk_fn, chunk_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b88e6729",
   "metadata": {},
   "source": [
    "## Generate commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2692b3fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i_task = 0; task = task_info[i_task]\n",
    "for i_task,task in enumerate(task_info):\n",
    "\n",
    "    chunk_file = task['input_file']\n",
    "    output_fn = chunk_file.replace('.json','_results.json')\n",
    "\n",
    "    task['output_file'] = output_fn\n",
    "\n",
    "    if n_jobs > 1:\n",
    "        gpu_number = i_task % n_gpus\n",
    "    else:\n",
    "        gpu_number = default_gpu_number\n",
    "\n",
    "    if os.name == 'nt':\n",
    "        cuda_string = f'set CUDA_VISIBLE_DEVICES={gpu_number} & '\n",
    "    else:\n",
    "        cuda_string = f'CUDA_VISIBLE_DEVICES={gpu_number} '\n",
    "\n",
    "    checkpoint_frequency_string = ''\n",
    "    checkpoint_path_string = ''\n",
    "    checkpoint_filename = chunk_file.replace('.json','_checkpoint.json')\n",
    "\n",
    "    if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "        checkpoint_frequency_string = f'--checkpoint_frequency {checkpoint_frequency}'\n",
    "        checkpoint_path_string = '--checkpoint_path \"{}\"'.format(checkpoint_filename)\n",
    "\n",
    "    use_image_queue_string = ''\n",
    "    if (use_image_queue):\n",
    "        use_image_queue_string = '--use_image_queue'\n",
    "\n",
    "    ncores_string = ''\n",
    "    if (ncores > 1):\n",
    "        ncores_string = '--ncores {}'.format(ncores)\n",
    "\n",
    "    quiet_string = ''\n",
    "    if quiet_mode:\n",
    "        quiet_string = '--quiet'\n",
    "\n",
    "    image_size_string = ''\n",
    "    if image_size is not None:\n",
    "        image_size_string = '--image_size {}'.format(image_size)\n",
    "\n",
    "    # Generate the script to run MD\n",
    "\n",
    "    cmd = f'{cuda_string} python run_detector_batch.py \"{model_file}\" \"{chunk_file}\" \"{output_fn}\" {checkpoint_frequency_string} {checkpoint_path_string} {use_image_queue_string} {ncores_string} {quiet_string} {image_size_string}'\n",
    "\n",
    "    cmd_file = os.path.join(filename_base,'run_chunk_{}_gpu_{}{}'.format(str(i_task).zfill(2),\n",
    "                            str(gpu_number).zfill(2),script_extension))\n",
    "\n",
    "    with open(cmd_file,'w') as f:\n",
    "        f.write(cmd + '\\n')\n",
    "\n",
    "    st = os.stat(cmd_file)\n",
    "    os.chmod(cmd_file, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "    task['command'] = cmd\n",
    "    task['command_file'] = cmd_file\n",
    "\n",
    "    # Generate the script to resume from the checkpoint\n",
    "\n",
    "    resume_string = ' --resume_from_checkpoint \"{}\"'.format(checkpoint_filename)\n",
    "    resume_cmd = cmd + resume_string\n",
    "    resume_cmd_file = os.path.join(filename_base,'resume_chunk_{}_gpu_{}{}'.format(str(i_task).zfill(2),\n",
    "                            str(gpu_number).zfill(2),script_extension))\n",
    "\n",
    "    with open(resume_cmd_file,'w') as f:\n",
    "        f.write(resume_cmd + '\\n')\n",
    "\n",
    "    st = os.stat(resume_cmd_file)\n",
    "    os.chmod(resume_cmd_file, st.st_mode | stat.S_IEXEC)\n",
    "\n",
    "    task['resume_command'] = resume_cmd\n",
    "    task['resume_command_file'] = resume_cmd_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc38125f",
   "metadata": {},
   "source": [
    "## Run the tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92964d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "I strongly prefer to manually run the scripts we just generated, but this cell demonstrates\n",
    "how one would invoke run_detector_batch programmatically.  Normally when I run manually on\n",
    "a multi-GPU machine, I run the scripts in N separate shells, one for each GPU.  This programmatic\n",
    "approach does not yet know how to do something like that, so all chunks will run serially.\n",
    "This is a no-op if you're on a single-GPU machine.\n",
    "\"\"\"\n",
    "\n",
    "if False:\n",
    "\n",
    "    #%%% Run the tasks (commented out)\n",
    "\n",
    "    # i_task = 0; task = task_info[i_task]\n",
    "    for i_task,task in enumerate(task_info):\n",
    "\n",
    "        chunk_file = task['input_file']\n",
    "        output_fn = task['output_file']\n",
    "\n",
    "        checkpoint_filename = chunk_file.replace('.json','_checkpoint.json')\n",
    "\n",
    "        if json_threshold is not None:\n",
    "            confidence_threshold = json_threshold\n",
    "        else:\n",
    "            confidence_threshold = DEFAULT_OUTPUT_CONFIDENCE_THRESHOLD\n",
    "\n",
    "        if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "            cp_freq_arg = checkpoint_frequency\n",
    "        else:\n",
    "            cp_freq_arg = -1\n",
    "\n",
    "        start_time = time.time()\n",
    "        results = load_and_run_detector_batch(model_file=model_file,\n",
    "                                              image_file_names=chunk_file,\n",
    "                                              checkpoint_path=checkpoint_filename,\n",
    "                                              confidence_threshold=confidence_threshold,\n",
    "                                              checkpoint_frequency=cp_freq_arg,\n",
    "                                              results=None,\n",
    "                                              n_cores=ncores,\n",
    "                                              use_image_queue=use_image_queue,\n",
    "                                              quiet=quiet_mode,\n",
    "                                              image_size=image_size)\n",
    "        elapsed = time.time() - start_time\n",
    "\n",
    "        print('Task {}: finished inference for {} images in {}'.format(\n",
    "            i_task, len(results),humanfriendly.format_timespan(elapsed)))\n",
    "\n",
    "        # This will write absolute paths to the file, we'll fix this later\n",
    "        write_results_to_file(results, output_fn, detector_file=model_file)\n",
    "\n",
    "        if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "            if os.path.isfile(checkpoint_filename):\n",
    "                os.remove(checkpoint_filename)\n",
    "                print('Deleted checkpoint file {}'.format(checkpoint_filename))\n",
    "\n",
    "    # ...for each chunk\n",
    "\n",
    "# ...if False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6824f209",
   "metadata": {},
   "source": [
    "## Load results, look for failed or missing images in each task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "525ae2ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_total_failures = 0\n",
    "\n",
    "# i_task = 0; task = task_info[i_task]\n",
    "for i_task,task in enumerate(task_info):\n",
    "\n",
    "    chunk_file = task['input_file']\n",
    "    output_file = task['output_file']\n",
    "\n",
    "    with open(chunk_file,'r') as f:\n",
    "        task_images = json.load(f)\n",
    "    with open(output_file,'r') as f:\n",
    "        task_results = json.load(f)\n",
    "\n",
    "    task_images_set = set(task_images)\n",
    "    filename_to_results = {}\n",
    "\n",
    "    n_task_failures = 0\n",
    "\n",
    "    # im = task_results['images'][0]\n",
    "    for im in task_results['images']:\n",
    "        assert im['file'].startswith(input_path)\n",
    "        assert im['file'] in task_images_set\n",
    "        filename_to_results[im['file']] = im\n",
    "        if 'failure' in im:\n",
    "            assert im['failure'] is not None\n",
    "            n_task_failures += 1\n",
    "\n",
    "    task['n_failures'] = n_task_failures\n",
    "    task['results'] = task_results\n",
    "\n",
    "    for fn in task_images:\n",
    "        assert fn in filename_to_results\n",
    "\n",
    "    n_total_failures += n_task_failures\n",
    "\n",
    "# ...for each task\n",
    "\n",
    "assert n_total_failures < max_tolerable_failed_images,\\\n",
    "    '{} failures (max tolerable set to {})'.format(n_total_failures,\n",
    "                                                   max_tolerable_failed_images)\n",
    "\n",
    "print('Processed all {} images with {} failures'.format(\n",
    "    len(all_images),n_total_failures))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7265870b",
   "metadata": {},
   "source": [
    "## Merge results files and make images relative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f2c7060",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "combined_results = None\n",
    "\n",
    "for i_task,task in enumerate(task_info):\n",
    "\n",
    "    if i_task == 0:\n",
    "        combined_results = copy.deepcopy(task['results'])\n",
    "        combined_results['images'] = copy.deepcopy(task['results']['images'])\n",
    "        continue\n",
    "    task_results = task['results']\n",
    "    assert task_results['info']['format_version'] == combined_results['info']['format_version']\n",
    "    assert task_results['detection_categories'] == combined_results['detection_categories']\n",
    "    combined_results['images'].extend(copy.deepcopy(task_results['images']))\n",
    "\n",
    "assert len(combined_results['images']) == len(all_images), \\\n",
    "    'Expected {} images in combined results, found {}'.format(\n",
    "        len(all_images),len(combined_results['images']))\n",
    "\n",
    "result_filenames = [im['file'] for im in combined_results['images']]\n",
    "assert len(combined_results['images']) == len(set(result_filenames))\n",
    "\n",
    "# im = combined_results['images'][0]\n",
    "for im in combined_results['images']:\n",
    "    assert im['file'].startswith(input_path + os.path.sep)\n",
    "    im['file']= im['file'].replace(input_path + os.path.sep,'',1)\n",
    "\n",
    "combined_api_output_file = os.path.join(\n",
    "    combined_api_output_folder,\n",
    "    '{}_detections.json'.format(base_task_name))\n",
    "\n",
    "with open(combined_api_output_file,'w') as f:\n",
    "    json.dump(combined_results,f,indent=2)\n",
    "\n",
    "print('Wrote results to {}'.format(combined_api_output_file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2bbdc12",
   "metadata": {},
   "source": [
    "## Post-processing (no ground truth)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5aeab0",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_animals_only = False\n",
    "\n",
    "options = PostProcessingOptions()\n",
    "options.image_base_dir = input_path\n",
    "options.include_almost_detections = True\n",
    "options.num_images_to_sample = 7500\n",
    "options.confidence_threshold = 0.2\n",
    "options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05\n",
    "options.ground_truth_json_file = None\n",
    "options.separate_detections_by_category = True\n",
    "# options.sample_seed = 0\n",
    "\n",
    "options.parallelize_rendering = True\n",
    "options.parallelize_rendering_n_cores = default_workers_for_parallel_tasks\n",
    "options.parallelize_rendering_with_threads = parallelization_defaults_to_threads\n",
    "\n",
    "if render_animals_only:\n",
    "    # Omit some pages from the output, useful when animals are rare\n",
    "    options.rendering_bypass_sets = ['detections_person','detections_vehicle',\n",
    "                                     'detections_person_vehicle','non_detections']\n",
    "\n",
    "output_base = os.path.join(postprocessing_output_folder,\n",
    "    base_task_name + '_{:.3f}'.format(options.confidence_threshold))\n",
    "if render_animals_only:\n",
    "    output_base = output_base + '_animals_only'\n",
    "\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "print('Processing to {}'.format(output_base))\n",
    "\n",
    "options.api_output_file = combined_api_output_file\n",
    "options.output_dir = output_base\n",
    "ppresults = process_batch_results(options)\n",
    "html_output_file = ppresults.output_html_file\n",
    "path_utils.open_file(html_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3763f231",
   "metadata": {},
   "source": [
    "## RDE (sample directory collapsing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88f59550",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_overflow_folders(relativePath):\n",
    "\n",
    "    import re\n",
    "\n",
    "    # In this example, the camera created folders called \"100EK113\", \"101EK113\", etc., for every N images\n",
    "    pat = '\\/\\d+EK\\d+\\/'\n",
    "\n",
    "    relativePath = relativePath.replace('\\\\','/')\n",
    "    relativePath = re.sub(pat,'/',relativePath)\n",
    "    dirName = os.path.dirname(relativePath)\n",
    "\n",
    "    return dirName\n",
    "\n",
    "if False:\n",
    "\n",
    "    pass\n",
    "\n",
    "    #%%\n",
    "\n",
    "    relativePath = 'a/b/c/d/100EK113/blah.jpg'\n",
    "    print(remove_overflow_folders(relativePath))\n",
    "\n",
    "    #%%\n",
    "\n",
    "    with open(combined_api_output_file,'r') as f:\n",
    "        d = json.load(f)\n",
    "    image_filenames = [im['file'] for im in d['images']]\n",
    "\n",
    "    #%%\n",
    "\n",
    "    dirNames = set()\n",
    "\n",
    "    # relativePath = image_filenames[0]\n",
    "    for relativePath in tqdm(image_filenames):\n",
    "        dirName = remove_overflow_folders(relativePath)\n",
    "        dirNames.add(dirName)\n",
    "\n",
    "    dirNames = list(dirNames)\n",
    "    dirNames.sort()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022052da",
   "metadata": {},
   "source": [
    "## Repeat detection elimination, phase 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d3abb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deliberately leaving these imports here, rather than at the top, because this\n",
    "# cell is not typically executed\n",
    "from api.batch_processing.postprocessing.repeat_detection_elimination import repeat_detections_core\n",
    "import path_utils\n",
    "task_index = 0\n",
    "\n",
    "options = repeat_detections_core.RepeatDetectionOptions()\n",
    "\n",
    "options.confidenceMin = 0.15\n",
    "options.confidenceMax = 1.01\n",
    "options.iouThreshold = 0.85\n",
    "options.occurrenceThreshold = 20\n",
    "options.maxSuspiciousDetectionSize = 0.2\n",
    "# options.minSuspiciousDetectionSize = 0.05\n",
    "\n",
    "options.parallelizationUsesThreads = parallelization_defaults_to_threads\n",
    "options.nWorkers = default_workers_for_parallel_tasks\n",
    "\n",
    "# This will cause a very light gray box to get drawn around all the detections\n",
    "# we're *not* considering as suspicious.\n",
    "options.bRenderOtherDetections = True\n",
    "options.otherDetectionsThreshold = options.confidenceMin\n",
    "\n",
    "# options.lineThickness = 5\n",
    "# options.boxExpansion = 8\n",
    "\n",
    "# To invoke custom collapsing of folders for a particular manufacturer's naming scheme\n",
    "# options.customDirNameFunction = remove_overflow_folders\n",
    "\n",
    "options.bRenderHtml = False\n",
    "options.imageBase = input_path\n",
    "rde_string = 'rde_{:.2f}_{:.2f}_{}_{:.2f}'.format(\n",
    "    options.confidenceMin, options.iouThreshold,\n",
    "    options.occurrenceThreshold, options.maxSuspiciousDetectionSize)\n",
    "options.outputBase = os.path.join(filename_base, rde_string + '_task_{}'.format(task_index))\n",
    "options.filenameReplacements = None # {'':''}\n",
    "\n",
    "# Exclude people and vehicles from RDE\n",
    "# options.excludeClasses = [2,3]\n",
    "\n",
    "# options.maxImagesPerFolder = 50000\n",
    "# options.includeFolders = ['a/b/c']\n",
    "# options.excludeFolder = ['a/b/c']\n",
    "\n",
    "options.debugMaxDir = -1\n",
    "options.debugMaxRenderDir = -1\n",
    "options.debugMaxRenderDetection = -1\n",
    "options.debugMaxRenderInstance = -1\n",
    "\n",
    "# Can be None, 'xsort', or 'clustersort'\n",
    "options.smartSort = 'xsort'\n",
    "\n",
    "suspiciousDetectionResults = repeat_detections_core.find_repeat_detections(combined_api_output_file,\n",
    "                                                                           None,\n",
    "                                                                           options)\n",
    "\n",
    "# import clipboard; clipboard.copy(os.path.dirname(suspiciousDetectionResults.filterFile))\n",
    "# path_utils.open_file(os.path.dirname(suspiciousDetectionResults.filterFile))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50bf0d6b",
   "metadata": {},
   "source": [
    "## Manual RDE step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7057a90a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## DELETE THE VALID DETECTIONS ##"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4efe3fce",
   "metadata": {},
   "source": [
    "## Re-filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "193352a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.batch_processing.postprocessing.repeat_detection_elimination import remove_repeat_detections\n",
    "\n",
    "filtered_output_filename = path_utils.insert_before_extension(combined_api_output_file, 'filtered_{}'.format(rde_string))\n",
    "\n",
    "remove_repeat_detections.remove_repeat_detections(\n",
    "    inputFile=combined_api_output_file,\n",
    "    outputFile=filtered_output_filename,\n",
    "    filteringDir=os.path.dirname(suspiciousDetectionResults.filterFile)\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bf10a47",
   "metadata": {},
   "source": [
    "## Post-processing (post-RDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0543493",
   "metadata": {},
   "outputs": [],
   "source": [
    "render_animals_only = False\n",
    "\n",
    "options = PostProcessingOptions()\n",
    "options.image_base_dir = input_path\n",
    "options.include_almost_detections = True\n",
    "options.num_images_to_sample = 7500\n",
    "options.confidence_threshold = 0.2\n",
    "options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05\n",
    "options.ground_truth_json_file = None\n",
    "options.separate_detections_by_category = True\n",
    "# options.sample_seed = 0\n",
    "\n",
    "options.parallelize_rendering = True\n",
    "options.parallelize_rendering_n_cores = default_workers_for_parallel_tasks\n",
    "options.parallelize_rendering_with_threads = parallelization_defaults_to_threads\n",
    "\n",
    "if render_animals_only:\n",
    "    # Omit some pages from the output, useful when animals are rare\n",
    "    options.rendering_bypass_sets = ['detections_person','detections_vehicle',\n",
    "                                      'detections_person_vehicle','non_detections']\n",
    "\n",
    "output_base = os.path.join(postprocessing_output_folder,\n",
    "    base_task_name + '_{}_{:.3f}'.format(rde_string, options.confidence_threshold))\n",
    "\n",
    "if render_animals_only:\n",
    "    output_base = output_base + '_render_animals_only'\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "\n",
    "print('Processing post-RDE to {}'.format(output_base))\n",
    "\n",
    "options.api_output_file = filtered_output_filename\n",
    "options.output_dir = output_base\n",
    "ppresults = process_batch_results(options)\n",
    "html_output_file = ppresults.output_html_file\n",
    "\n",
    "path_utils.open_file(html_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80818026",
   "metadata": {},
   "source": [
    "## Run MegaClassifier (actually, write out a script that runs MegaClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39f367a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name_short = 'megaclassifier'\n",
    "threshold_str = '0.15' # 0.6\n",
    "classifier_name = 'megaclassifier_v0.1_efficientnet-b3'\n",
    "\n",
    "organization_name = organization_name_short\n",
    "job_name = base_task_name\n",
    "input_filename = filtered_output_filename # combined_api_output_file\n",
    "input_files = [input_filename]\n",
    "image_base = input_path\n",
    "crop_path = os.path.join(os.path.expanduser('~/crops'),job_name + '_crops')\n",
    "output_base = combined_api_output_folder\n",
    "device_id = 0\n",
    "\n",
    "output_file = os.path.join(filename_base,'run_{}_'.format(classifier_name_short) + job_name + script_extension)\n",
    "\n",
    "classifier_base = os.path.expanduser('~/models/camera_traps/megaclassifier/v0.1/')\n",
    "assert os.path.isdir(classifier_base)\n",
    "\n",
    "checkpoint_path = os.path.join(classifier_base,'v0.1_efficientnet-b3_compiled.pt')\n",
    "assert os.path.isfile(checkpoint_path)\n",
    "\n",
    "classifier_categories_path = os.path.join(classifier_base,'v0.1_index_to_name.json')\n",
    "assert os.path.isfile(classifier_categories_path)\n",
    "\n",
    "target_mapping_path = os.path.join(classifier_base,'idfg_to_megaclassifier_labels.json')\n",
    "assert os.path.isfile(target_mapping_path)\n",
    "\n",
    "classifier_output_suffix = '_megaclassifier_output.csv.gz'\n",
    "final_output_suffix = '_megaclassifier.json'\n",
    "\n",
    "n_threads_str = str(default_workers_for_parallel_tasks)\n",
    "image_size_str = '300'\n",
    "batch_size_str = '64'\n",
    "num_workers_str = str(default_workers_for_parallel_tasks)\n",
    "classification_threshold_str = '0.05'\n",
    "\n",
    "logdir = filename_base\n",
    "\n",
    "# This is just passed along to the metadata in the output file, it has no impact\n",
    "# on how the classification scripts run.\n",
    "typical_classification_threshold_str = '0.75'\n",
    "\n",
    "##%% Set up environment\n",
    "\n",
    "commands = []\n",
    "# commands.append('cd CameraTraps/classification\\n')\n",
    "# commands.append('conda activate cameratraps-classifier\\n')\n",
    "\n",
    "##%% Crop images\n",
    "\n",
    "commands.append('\\n' + scc + ' Cropping ' + scc + '\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    crop_cmd = ''\n",
    "\n",
    "    crop_comment = '\\n' + scc + ' Cropping {}\\n'.format(fn)\n",
    "    crop_cmd += crop_comment\n",
    "\n",
    "    crop_cmd += \"python crop_detections.py \" + slcc + \"\\n\" + \\\n",
    "    \t ' \"' + input_file_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' \"' + crop_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' ' + '--images-dir \"' + image_base + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--threshold \"' + threshold_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--square-crops ' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--threads \"' + n_threads_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--logdir \"' + logdir + '\"' + '\\n' + \\\n",
    "         ' ' + '\\n'\n",
    "    crop_cmd = '{}'.format(crop_cmd)\n",
    "    commands.append(crop_cmd)\n",
    "\n",
    "\n",
    "##%% Run classifier\n",
    "\n",
    "commands.append('\\n' + scc + ' Classifying ' + scc + '\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "\n",
    "    classify_cmd = ''\n",
    "\n",
    "    classify_comment = '\\n' + scc + ' Classifying {}\\n'.format(fn)\n",
    "    classify_cmd += classify_comment\n",
    "\n",
    "    classify_cmd += \"python run_classifier.py \" + slcc + \"\\n\" + \\\n",
    "    \t ' \"' + checkpoint_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' \"' + crop_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' \"' + classifier_output_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' ' + '--detections-json \"' + input_file_path + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--classifier-categories \"' + classifier_categories_path + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--image-size \"' + image_size_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--batch-size \"' + batch_size_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--num-workers \"' + num_workers_str + '\"' + ' ' + slcc + '\\n'\n",
    "\n",
    "    if device_id is not None:\n",
    "        classify_cmd += ' ' + '--device {}'.format(device_id)\n",
    "\n",
    "    classify_cmd += '\\n\\n'\n",
    "    classify_cmd = '{}'.format(classify_cmd)\n",
    "    commands.append(classify_cmd)\n",
    "\n",
    "\n",
    "##%% Remap classifier outputs\n",
    "\n",
    "commands.append('\\n' + scc + ' Remapping ' + scc + '\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "    classifier_output_path_remapped = \\\n",
    "        classifier_output_path.replace(\".csv.gz\",\"_remapped.csv.gz\")\n",
    "    assert not (classifier_output_path == classifier_output_path_remapped)\n",
    "\n",
    "    output_label_index = classifier_output_path_remapped.replace(\n",
    "        \"_remapped.csv.gz\",\"_label_index_remapped.json\")\n",
    "\n",
    "    remap_cmd = ''\n",
    "\n",
    "    remap_comment = '\\n' + scc + ' Remapping {}\\n'.format(fn)\n",
    "    remap_cmd += remap_comment\n",
    "\n",
    "    remap_cmd += \"python aggregate_classifier_probs.py \" + slcc + \"\\n\" + \\\n",
    "        ' \"' + classifier_output_path + '\" ' + slcc + '\\n' + \\\n",
    "        ' ' + '--target-mapping \"' + target_mapping_path + '\"' + ' ' + slcc + '\\n' + \\\n",
    "        ' ' + '--output-csv \"' + classifier_output_path_remapped + '\"' + ' ' + slcc + '\\n' + \\\n",
    "        ' ' + '--output-label-index \"' + output_label_index + '\"' \\\n",
    "        '\\n'\n",
    "\n",
    "    remap_cmd = '{}'.format(remap_cmd)\n",
    "    commands.append(remap_cmd)\n",
    "\n",
    "\n",
    "##%% Merge classification and detection outputs\n",
    "\n",
    "commands.append('\\n' + scc + ' Merging ' + scc + '\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "\n",
    "    classifier_output_path_remapped = \\\n",
    "        classifier_output_path.replace(\".csv.gz\",\"_remapped.csv.gz\")\n",
    "\n",
    "    output_label_index = classifier_output_path_remapped.replace(\n",
    "        \"_remapped.csv.gz\",\"_label_index_remapped.json\")\n",
    "\n",
    "    final_output_path = os.path.join(output_base,\n",
    "                                     os.path.basename(classifier_output_path)).\\\n",
    "        replace(classifier_output_suffix,\n",
    "        final_output_suffix)\n",
    "    final_output_path = final_output_path.replace('_detections','')\n",
    "    final_output_path = final_output_path.replace('_crops','')\n",
    "    final_output_path_mc = final_output_path\n",
    "\n",
    "    merge_cmd = ''\n",
    "\n",
    "    merge_comment = '\\n' + scc + ' Merging {}\\n'.format(fn)\n",
    "    merge_cmd += merge_comment\n",
    "\n",
    "    merge_cmd += \"python merge_classification_detection_output.py \" + slcc + \"\\n\" + \\\n",
    "    \t ' \"' + classifier_output_path_remapped + '\" ' + slcc + '\\n' + \\\n",
    "         ' \"' + output_label_index + '\" ' + slcc + '\\n' + \\\n",
    "         ' ' + '--output-json \"' + final_output_path + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--detection-json \"' + input_file_path + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--classifier-name \"' + classifier_name + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--threshold \"' + classification_threshold_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--typical-confidence-threshold \"' + typical_classification_threshold_str + '\"' + '\\n' + \\\n",
    "         '\\n'\n",
    "    merge_cmd = '{}'.format(merge_cmd)\n",
    "    commands.append(merge_cmd)\n",
    "\n",
    "\n",
    "##%% Write  out classification script\n",
    "\n",
    "with open(output_file,'w') as f:\n",
    "    for s in commands:\n",
    "        f.write('{}'.format(s))\n",
    "\n",
    "import stat\n",
    "st = os.stat(output_file)\n",
    "os.chmod(output_file, st.st_mode | stat.S_IEXEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85b55368",
   "metadata": {},
   "source": [
    "## Run a non-MegaClassifier classifier (i.e., a classifier with no output mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bcee8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier_name_short = 'idfgclassifier'\n",
    "threshold_str = '0.15' # 0.6\n",
    "classifier_name = 'idfg_classifier_ckpt_14_compiled'\n",
    "\n",
    "organization_name = organization_name_short\n",
    "job_name = base_task_name\n",
    "input_filename = filtered_output_filename # combined_api_output_file\n",
    "input_files = [input_filename]\n",
    "image_base = input_path\n",
    "crop_path = os.path.join(os.path.expanduser('~/crops'),job_name + '_crops')\n",
    "output_base = combined_api_output_folder\n",
    "device_id = 1\n",
    "\n",
    "output_file = os.path.join(filename_base,'run_{}_'.format(classifier_name_short) + job_name +  script_extension)\n",
    "\n",
    "classifier_base = os.path.expanduser('~/models/camera_traps/idfg_classifier/idfg_classifier_20200905_042558')\n",
    "assert os.path.isdir(classifier_base)\n",
    "\n",
    "checkpoint_path = os.path.join(classifier_base,'idfg_classifier_ckpt_14_compiled.pt')\n",
    "assert os.path.isfile(checkpoint_path)\n",
    "\n",
    "classifier_categories_path = os.path.join(classifier_base,'label_index.json')\n",
    "assert os.path.isfile(classifier_categories_path)\n",
    "\n",
    "classifier_output_suffix = '_{}_output.csv.gz'.format(classifier_name_short)\n",
    "final_output_suffix = '_{}.json'.format(classifier_name_short)\n",
    "\n",
    "threshold_str = '0.65'\n",
    "n_threads_str = str(default_workers_for_parallel_tasks)\n",
    "image_size_str = '300'\n",
    "batch_size_str = '64'\n",
    "num_workers_str = str(default_workers_for_parallel_tasks)\n",
    "logdir = filename_base\n",
    "\n",
    "classification_threshold_str = '0.05'\n",
    "\n",
    "# This is just passed along to the metadata in the output file, it has no impact\n",
    "# on how the classification scripts run.\n",
    "typical_classification_threshold_str = '0.75'\n",
    "\n",
    "\n",
    "##%% Set up environment\n",
    "\n",
    "commands = []\n",
    "\n",
    "\n",
    "##%% Crop images\n",
    "\n",
    "commands.append('\\n' + scc + ' Cropping ' + scc + '\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    crop_cmd = ''\n",
    "\n",
    "    crop_comment = '\\n' + scc + ' Cropping {}\\n'.format(fn)\n",
    "    crop_cmd += crop_comment\n",
    "\n",
    "    crop_cmd += \"python crop_detections.py \" + slcc + \"\\n\" + \\\n",
    "    \t ' \"' + input_file_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' \"' + crop_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' ' + '--images-dir \"' + image_base + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--threshold \"' + threshold_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--square-crops ' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--threads \"' + n_threads_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--logdir \"' + logdir + '\"' + '\\n' + \\\n",
    "         '\\n'\n",
    "    crop_cmd = '{}'.format(crop_cmd)\n",
    "    commands.append(crop_cmd)\n",
    "\n",
    "\n",
    "##%% Run classifier\n",
    "\n",
    "commands.append('\\n' + scc + ' Classifying ' + scc + '\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "\n",
    "    classify_cmd = ''\n",
    "\n",
    "    classify_comment = '\\n' + scc + ' Classifying {}\\n'.format(fn)\n",
    "    classify_cmd += classify_comment\n",
    "\n",
    "    classify_cmd += \"python run_classifier.py \" + slcc + \"\\n\" + \\\n",
    "    \t ' \"' + checkpoint_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' \"' + crop_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' \"' + classifier_output_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' ' + '--detections-json \"' + input_file_path + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--classifier-categories \"' + classifier_categories_path + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--image-size \"' + image_size_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--batch-size \"' + batch_size_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--num-workers \"' + num_workers_str + '\"' + ' ' + slcc + '\\n'\n",
    "\n",
    "    if device_id is not None:\n",
    "        classify_cmd += ' ' + '--device {}'.format(device_id)\n",
    "\n",
    "    classify_cmd += '\\n\\n'\n",
    "    classify_cmd = '{}'.format(classify_cmd)\n",
    "    commands.append(classify_cmd)\n",
    "\n",
    "\n",
    "##%% Merge classification and detection outputs\n",
    "\n",
    "commands.append('\\n' + scc + ' Merging ' + scc + '\\n')\n",
    "\n",
    "# fn = input_files[0]\n",
    "for fn in input_files:\n",
    "\n",
    "    input_file_path = fn\n",
    "    classifier_output_path = crop_path + classifier_output_suffix\n",
    "    final_output_path = os.path.join(output_base,\n",
    "                                     os.path.basename(classifier_output_path)).\\\n",
    "                                     replace(classifier_output_suffix,\n",
    "                                     final_output_suffix)\n",
    "    final_output_path = final_output_path.replace('_detections','')\n",
    "    final_output_path = final_output_path.replace('_crops','')\n",
    "    final_output_path_ic = final_output_path\n",
    "\n",
    "    merge_cmd = ''\n",
    "\n",
    "    merge_comment = '\\n' + scc + ' Merging {}\\n'.format(fn)\n",
    "    merge_cmd += merge_comment\n",
    "\n",
    "    merge_cmd += \"python merge_classification_detection_output.py \" + slcc + \"\\n\" + \\\n",
    "    \t ' \"' + classifier_output_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' \"' + classifier_categories_path + '\" ' + slcc + '\\n' + \\\n",
    "         ' ' + '--output-json \"' + final_output_path_ic + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--detection-json \"' + input_file_path + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--classifier-name \"' + classifier_name + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--threshold \"' + classification_threshold_str + '\"' + ' ' + slcc + '\\n' + \\\n",
    "         ' ' + '--typical-confidence-threshold \"' + typical_classification_threshold_str + '\"' + '\\n' + \\\n",
    "         '\\n'\n",
    "    merge_cmd = '{}'.format(merge_cmd)\n",
    "    commands.append(merge_cmd)\n",
    "\n",
    "\n",
    "##%% Write everything out\n",
    "\n",
    "with open(output_file,'w') as f:\n",
    "    for s in commands:\n",
    "        f.write('{}'.format(s))\n",
    "\n",
    "import stat\n",
    "st = os.stat(output_file)\n",
    "os.chmod(output_file, st.st_mode | stat.S_IEXEC)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2895494c",
   "metadata": {},
   "source": [
    "## Run the classifier(s) via the .sh script(s) write just wrote"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf5f9ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42b7cc3c",
   "metadata": {},
   "source": [
    "## Within-image classification smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88bf4ad6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#\n",
    "# Only count detections with a classification confidence threshold above\n",
    "# *classification_confidence_threshold*, which in practice means we're only\n",
    "# looking at one category per detection.\n",
    "#\n",
    "# If an image has at least *min_detections_above_threshold* such detections\n",
    "# in the most common category, and no more than *max_detections_secondary_class*\n",
    "# in the second-most-common category, flip all detections to the most common\n",
    "# category.\n",
    "#\n",
    "# Optionally treat some classes as particularly unreliable, typically used to overwrite an\n",
    "# \"other\" class.\n",
    "#\n",
    "# This cell also removes everything but the non-dominant classification for each detection.\n",
    "#\n",
    "\n",
    "# How many detections do we need above the classification threshold to determine a dominant category\n",
    "# for an image?\n",
    "min_detections_above_threshold = 4\n",
    "\n",
    "# Even if we have a dominant class, if a non-dominant class has at least this many classifications\n",
    "# in an image, leave them alone.\n",
    "max_detections_secondary_class = 3\n",
    "\n",
    "# If the dominant class has at least this many classifications, overwrite \"other\" classifications\n",
    "min_detections_to_overwrite_other = 2\n",
    "other_category_names = ['other']\n",
    "\n",
    "# What confidence threshold should we use for assessing the dominant category in an image?\n",
    "classification_confidence_threshold = 0.6\n",
    "\n",
    "# Which classifications should we even bother over-writing?\n",
    "classification_overwrite_threshold = 0.3\n",
    "\n",
    "# Detection confidence threshold for things we count when determining a dominant class\n",
    "detection_confidence_threshold = 0.2\n",
    "\n",
    "# Which detections should we even bother over-writing?\n",
    "detection_overwrite_threshold = 0.05\n",
    "\n",
    "classification_detection_files = [\n",
    "    final_output_path_mc,final_output_path_ic\n",
    "    ]\n",
    "\n",
    "assert all([os.path.isfile(fn) for fn in classification_detection_files])\n",
    "\n",
    "smoothed_classification_files = []\n",
    "\n",
    "for final_output_path in classification_detection_files:\n",
    "\n",
    "    classifier_output_path = final_output_path\n",
    "    classifier_output_path_within_image_smoothing = classifier_output_path.replace(\n",
    "        '.json','_within_image_smoothing.json')\n",
    "\n",
    "    with open(classifier_output_path,'r') as f:\n",
    "        d = json.load(f)\n",
    "\n",
    "    from collections import defaultdict\n",
    "\n",
    "    category_name_to_id = {d['classification_categories'][k]:k for k in d['classification_categories']}\n",
    "    other_category_ids = []\n",
    "    for s in other_category_names:\n",
    "        if s in category_name_to_id:\n",
    "            other_category_ids.append(category_name_to_id[s])\n",
    "        else:\n",
    "            print('Warning: \"other\" category {} not present in file {}'.format(\n",
    "                s,classifier_output_path))\n",
    "\n",
    "    n_other_classifications_changed = 0\n",
    "    n_other_images_changed = 0\n",
    "\n",
    "    n_detections_flipped = 0\n",
    "    n_images_changed = 0\n",
    "\n",
    "    # Before we do anything else, get rid of everything but the top classification\n",
    "    # for each detection.\n",
    "    for im in tqdm(d['images']):\n",
    "\n",
    "        if 'detections' not in im or im['detections'] is None or len(im['detections']) == 0:\n",
    "            continue\n",
    "\n",
    "        detections = im['detections']\n",
    "\n",
    "        for det in detections:\n",
    "\n",
    "            if 'classifications' not in det or len(det['classifications']) == 0:\n",
    "                continue\n",
    "\n",
    "            classification_confidence_values = [c[1] for c in det['classifications']]\n",
    "            assert is_list_sorted(classification_confidence_values,reverse=True)\n",
    "            det['classifications'] = [det['classifications'][0]]\n",
    "\n",
    "        # ...for each detection in this image\n",
    "\n",
    "    # ...for each image\n",
    "\n",
    "    # im = d['images'][0]\n",
    "    for im in tqdm(d['images']):\n",
    "\n",
    "        if 'detections' not in im or im['detections'] is None or len(im['detections']) == 0:\n",
    "            continue\n",
    "\n",
    "        detections = im['detections']\n",
    "\n",
    "        category_to_count = defaultdict(int)\n",
    "        for det in detections:\n",
    "            if ('classifications' in det) and (det['conf'] >= detection_confidence_threshold):\n",
    "                for c in det['classifications']:\n",
    "                    if c[1] >= classification_confidence_threshold:\n",
    "                        category_to_count[c[0]] += 1\n",
    "                # ...for each classification\n",
    "            # ...if there are classifications for this detection\n",
    "        # ...for each detection\n",
    "\n",
    "        if len(category_to_count) <= 1:\n",
    "            continue\n",
    "\n",
    "        category_to_count = {k: v for k, v in sorted(category_to_count.items(),\n",
    "                                                     key=lambda item: item[1],\n",
    "                                                     reverse=True)}\n",
    "\n",
    "        keys = list(category_to_count.keys())\n",
    "\n",
    "        # Handle a quirky special case: if the most common category is \"other\" and\n",
    "        # it's \"tied\" with the second-most-common category, swap them\n",
    "        if (len(keys) > 1) and \\\n",
    "            (keys[0] in other_category_ids) and \\\n",
    "            (keys[1] not in other_category_ids) and \\\n",
    "            (category_to_count[keys[0]] == category_to_count[keys[1]]):\n",
    "                keys[1], keys[0] = keys[0], keys[1]\n",
    "\n",
    "        max_count = category_to_count[keys[0]]\n",
    "        # secondary_count = category_to_count[keys[1]]\n",
    "        # The 'secondary count' is the most common non-other class\n",
    "        secondary_count = 0\n",
    "        for i_key in range(1,len(keys)):\n",
    "            if keys[i_key] not in other_category_ids:\n",
    "                secondary_count = category_to_count[keys[i_key]]\n",
    "                break\n",
    "\n",
    "        most_common_category = keys[0]\n",
    "\n",
    "        assert max_count >= secondary_count\n",
    "\n",
    "        # If we have at least *min_detections_to_overwrite_other* in a category that isn't\n",
    "        # \"other\", change all \"other\" classifications to that category\n",
    "        if max_count >= min_detections_to_overwrite_other and \\\n",
    "            most_common_category not in other_category_ids:\n",
    "\n",
    "            other_change_made = False\n",
    "\n",
    "            for det in detections:\n",
    "\n",
    "                if ('classifications' in det) and (det['conf'] >= detection_overwrite_threshold):\n",
    "\n",
    "                    for c in det['classifications']:\n",
    "\n",
    "                        if c[1] >= classification_overwrite_threshold and \\\n",
    "                            c[0] in other_category_ids:\n",
    "\n",
    "                            n_other_classifications_changed += 1\n",
    "                            other_change_made = True\n",
    "                            c[0] = most_common_category\n",
    "\n",
    "                    # ...for each classification\n",
    "\n",
    "                # ...if there are classifications for this detection\n",
    "\n",
    "            # ...for each detection\n",
    "\n",
    "            if other_change_made:\n",
    "                n_other_images_changed += 1\n",
    "\n",
    "        # ...if we should overwrite all \"other\" classifications\n",
    "\n",
    "        if max_count < min_detections_above_threshold:\n",
    "            continue\n",
    "\n",
    "        if secondary_count >= max_detections_secondary_class:\n",
    "            continue\n",
    "\n",
    "        # At this point, we know we have a dominant category; change all other above-threshold\n",
    "        # classifications to that category.  That category may have been \"other\", in which\n",
    "        # case we may have already made the relevant changes.\n",
    "\n",
    "        n_detections_flipped_this_image = 0\n",
    "\n",
    "        # det = detections[0]\n",
    "        for det in detections:\n",
    "\n",
    "            if ('classifications' in det) and (det['conf'] >= detection_overwrite_threshold):\n",
    "\n",
    "                for c in det['classifications']:\n",
    "                    if c[1] >= classification_overwrite_threshold and \\\n",
    "                        c[0] != most_common_category:\n",
    "\n",
    "                        c[0] = most_common_category\n",
    "                        n_detections_flipped += 1\n",
    "                        n_detections_flipped_this_image += 1\n",
    "\n",
    "                # ...for each classification\n",
    "\n",
    "            # ...if there are classifications for this detection\n",
    "\n",
    "        # ...for each detection\n",
    "\n",
    "        if n_detections_flipped_this_image > 0:\n",
    "            n_images_changed += 1\n",
    "\n",
    "    # ...for each image\n",
    "\n",
    "    print('Classification smoothing: changed {} detections on {} images'.format(\n",
    "        n_detections_flipped,n_images_changed))\n",
    "\n",
    "    print('\"Other\" smoothing: changed {} detections on {} images'.format(\n",
    "          n_other_classifications_changed,n_other_images_changed))\n",
    "\n",
    "    with open(classifier_output_path_within_image_smoothing,'w') as f:\n",
    "        json.dump(d,f,indent=2)\n",
    "\n",
    "    print('Wrote results to:\\n{}'.format(classifier_output_path_within_image_smoothing))\n",
    "    smoothed_classification_files.append(classifier_output_path_within_image_smoothing)\n",
    "\n",
    "# ...for each file we want to smooth"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ac6a88",
   "metadata": {},
   "source": [
    "## Post-processing (post-classification, post-within-image-smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53094c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_detection_files = smoothed_classification_files\n",
    "\n",
    "assert all([os.path.isfile(fn) for fn in classification_detection_files])\n",
    "\n",
    "# classification_detection_file = classification_detection_files[1]\n",
    "for classification_detection_file in classification_detection_files:\n",
    "\n",
    "    options = PostProcessingOptions()\n",
    "    options.image_base_dir = input_path\n",
    "    options.include_almost_detections = True\n",
    "    options.num_images_to_sample = 10000\n",
    "    options.confidence_threshold = 0.2\n",
    "    options.classification_confidence_threshold = 0.75\n",
    "    options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05\n",
    "    options.ground_truth_json_file = None\n",
    "    options.separate_detections_by_category = True\n",
    "\n",
    "    options.parallelize_rendering = True\n",
    "    options.parallelize_rendering_n_cores = default_workers_for_parallel_tasks\n",
    "    options.parallelize_rendering_with_threads = parallelization_defaults_to_threads\n",
    "\n",
    "    folder_token = classification_detection_file.split(os.path.sep)[-1].replace('classifier.json','')\n",
    "\n",
    "    output_base = os.path.join(postprocessing_output_folder, folder_token + \\\n",
    "        base_task_name + '_{:.3f}'.format(options.confidence_threshold))\n",
    "    os.makedirs(output_base, exist_ok=True)\n",
    "    print('Processing {} to {}'.format(base_task_name, output_base))\n",
    "\n",
    "    options.api_output_file = classification_detection_file\n",
    "    options.output_dir = output_base\n",
    "    ppresults = process_batch_results(options)\n",
    "    path_utils.open_file(ppresults.output_html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56eba400",
   "metadata": {},
   "source": [
    "## Read EXIF data from all images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "511bd62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_management import read_exif\n",
    "exif_options = read_exif.ReadExifOptions()\n",
    "\n",
    "exif_options.verbose = False\n",
    "exif_options.n_workers = default_workers_for_parallel_tasks\n",
    "exif_options.use_threads = parallelization_defaults_to_threads\n",
    "exif_options.processing_library = 'pil'\n",
    "\n",
    "exif_results_file = os.path.join(filename_base,'exif_data.json')\n",
    "\n",
    "if os.path.isfile(exif_results_file):\n",
    "    print('Reading EXIF results from {}'.format(exif_results_file))\n",
    "    with open(exif_results_file,'r') as f:\n",
    "        exif_results = json.load(f)\n",
    "else:\n",
    "    exif_results = read_exif.read_exif_from_folder(input_path,\n",
    "                                                   output_file=exif_results_file,\n",
    "                                                   options=exif_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "654b5dfa",
   "metadata": {},
   "source": [
    "## Prepare COCO-camera-traps-compatible image objects for EXIF results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9c48bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dateutil\n",
    "import datetime\n",
    "import time\n",
    "\n",
    "def parse_date_from_exif_datetime(s):\n",
    "\n",
    "    # This is a standard format for EXIF datetime, and dateutil.parser\n",
    "    # doesn't handle it correctly.\n",
    "\n",
    "    # return dateutil.parser.parse(s)\n",
    "    return time.strptime(s, '%Y:%m:%d %H:%M:%S')\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "\n",
    "image_info = []\n",
    "\n",
    "# exif_result = exif_results[0]\n",
    "for exif_result in tqdm(exif_results):\n",
    "\n",
    "    im = {}\n",
    "\n",
    "    # Currently we assume that each leaf-node folder is a location\n",
    "    im['location'] = os.path.dirname(exif_result['file_name'])\n",
    "    im['file_name'] = exif_result['file_name']\n",
    "    im['id'] = im['file_name']\n",
    "    exif_dt = exif_result['exif_tags']['DateTime']\n",
    "    im['datetime'] = datetime.datetime.fromtimestamp(\n",
    "        time.mktime(parse_date_from_exif_datetime(exif_dt)))\n",
    "\n",
    "    # We collected this image this century, but not today, make sure the parsed datetime\n",
    "    # jives with that.\n",
    "    #\n",
    "    # The latter check is to make sure we don't repeat a particular pathological approach\n",
    "    # to datetime parsing, where dateutil parses time correctly, but swaps in the current\n",
    "    # date when it's not sure where the date is.\n",
    "    assert im['datetime'].year >= 2000\n",
    "    assert (now - im['datetime']).total_seconds() > 1*24*60*60\n",
    "\n",
    "    image_info.append(im)\n",
    "\n",
    "# ...for each exif image result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45f86553",
   "metadata": {},
   "source": [
    "## Assemble into sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7850246",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from data_management import cct_json_utils\n",
    "\n",
    "print('Assembling images into sequences')\n",
    "\n",
    "cct_json_utils.create_sequences(image_info)\n",
    "\n",
    "# Make a list of images appearing at each location\n",
    "sequence_to_images = defaultdict(list)\n",
    "\n",
    "# im = image_info[0]\n",
    "for im in tqdm(image_info):\n",
    "    sequence_to_images[im['seq_id']].append(im)\n",
    "\n",
    "all_sequences = list(sorted(sequence_to_images.keys()))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed4d983e",
   "metadata": {},
   "source": [
    "## Load classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3667544",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_level_smoothing_input_file = smoothed_classification_files[0]\n",
    "\n",
    "with open(sequence_level_smoothing_input_file,'r') as f:\n",
    "    d = json.load(f)\n",
    "\n",
    "# Map each filename to classification results for that file\n",
    "filename_to_results = {}\n",
    "\n",
    "for im in tqdm(d['images']):\n",
    "    filename_to_results[im['file'].replace('\\\\','/')] = im"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a358edc",
   "metadata": {},
   "source": [
    "## Smooth classification results over sequences (prep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3408934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ct_utils import is_list_sorted\n",
    "\n",
    "classification_category_id_to_name = d['classification_categories']\n",
    "classification_category_name_to_id = {v: k for k, v in classification_category_id_to_name.items()}\n",
    "\n",
    "class_names = list(classification_category_id_to_name.values())\n",
    "\n",
    "animal_detection_category = '1'\n",
    "assert(d['detection_categories'][animal_detection_category] == 'animal')\n",
    "\n",
    "other_category_names = set(['other'])\n",
    "other_category_ids = set([classification_category_name_to_id[s] for s in other_category_names])\n",
    "\n",
    "# These are the only classes to which we're going to switch other classifications\n",
    "category_names_to_smooth_to = set(['deer','elk','cow','canid','cat','bird','bear'])\n",
    "category_ids_to_smooth_to = set([classification_category_name_to_id[s] for s in category_names_to_smooth_to])\n",
    "assert all([s in class_names for s in category_names_to_smooth_to])\n",
    "\n",
    "# Only switch classifications to the dominant class if we see the dominant class at least\n",
    "# this many times\n",
    "min_dominant_class_classifications_above_threshold_for_class_smoothing = 5 # 2\n",
    "\n",
    "# If we see more than this many of a class that are above threshold, don't switch those\n",
    "# classifications to the dominant class.\n",
    "max_secondary_class_classifications_above_threshold_for_class_smoothing = 5\n",
    "\n",
    "# If the ratio between a dominant class and a secondary class count is greater than this,\n",
    "# regardless of the secondary class count, switch those classificaitons (i.e., ignore\n",
    "# max_secondary_class_classifications_above_threshold_for_class_smoothing).\n",
    "#\n",
    "# This may be different for different dominant classes, e.g. if we see lots of cows, they really\n",
    "# tend to be cows.  Less so for canids, so we set a higher \"override ratio\" for canids.\n",
    "min_dominant_class_ratio_for_secondary_override_table = {classification_category_name_to_id['cow']:2,None:3}\n",
    "\n",
    "# If there are at least this many classifications for the dominant class in a sequence,\n",
    "# regardless of what that class is, convert all 'other' classifications (regardless of\n",
    "# confidence) to that class.\n",
    "min_dominant_class_classifications_above_threshold_for_other_smoothing = 3 # 2\n",
    "\n",
    "# If there are at least this many classifications for the dominant class in a sequence,\n",
    "# regardless of what that class is, classify all previously-unclassified detections\n",
    "# as that class.\n",
    "min_dominant_class_classifications_above_threshold_for_unclassified_smoothing = 3 # 2\n",
    "\n",
    "# Only count classifications above this confidence level when determining the dominant\n",
    "# class, and when deciding whether to switch other classifications.\n",
    "classification_confidence_threshold = 0.6\n",
    "\n",
    "# Confidence values to use when we change a detection's classification (the\n",
    "# original confidence value is irrelevant at that point)\n",
    "flipped_other_confidence_value = 0.6\n",
    "flipped_class_confidence_value = 0.6\n",
    "flipped_unclassified_confidence_value = 0.6\n",
    "\n",
    "min_detection_confidence_for_unclassified_flipping = 0.15"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39a30fad",
   "metadata": {},
   "source": [
    "## Smooth classification results over sequences (supporting functions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641b860c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def results_for_sequence(images_this_sequence):\n",
    "    \"\"\"\n",
    "    Fetch MD results for every image in this sequence, based on the 'file_name' field\n",
    "    \"\"\"\n",
    "\n",
    "    results_this_sequence = []\n",
    "    for im in images_this_sequence:\n",
    "        fn = im['file_name']\n",
    "        results_this_image = filename_to_results[fn]\n",
    "        assert isinstance(results_this_image,dict)\n",
    "        results_this_sequence.append(results_this_image)\n",
    "\n",
    "    return results_this_sequence\n",
    "\n",
    "\n",
    "def top_classifications_for_sequence(images_this_sequence):\n",
    "    \"\"\"\n",
    "    Return all top-1 animal classifications for every detection in this\n",
    "    sequence, regardless of  confidence\n",
    "\n",
    "    May modify [images_this_sequence] (removing non-top-1 classifications)\n",
    "    \"\"\"\n",
    "\n",
    "    classifications_this_sequence = []\n",
    "\n",
    "    # im = images_this_sequence[0]\n",
    "    for im in images_this_sequence:\n",
    "\n",
    "        fn = im['file_name']\n",
    "        results_this_image = filename_to_results[fn]\n",
    "\n",
    "        if results_this_image['detections'] is None:\n",
    "            continue\n",
    "\n",
    "        # det = results_this_image['detections'][0]\n",
    "        for det in results_this_image['detections']:\n",
    "\n",
    "            # Only process animal detections\n",
    "            if det['category'] != animal_detection_category:\n",
    "                continue\n",
    "\n",
    "            # Only process detections with classification information\n",
    "            if 'classifications' not in det:\n",
    "                continue\n",
    "\n",
    "            # We only care about top-1 classifications, remove everything else\n",
    "            if len(det['classifications']) > 1:\n",
    "\n",
    "                # Make sure the list of classifications is already sorted by confidence\n",
    "                classification_confidence_values = [c[1] for c in det['classifications']]\n",
    "                assert is_list_sorted(classification_confidence_values,reverse=True)\n",
    "\n",
    "                # ...and just keep the first one\n",
    "                det['classifications'] = [det['classifications'][0]]\n",
    "\n",
    "            # Confidence values should be sorted within a detection; verify this, and ignore\n",
    "            top_classification = det['classifications'][0]\n",
    "\n",
    "            classifications_this_sequence.append(top_classification)\n",
    "\n",
    "        # ...for each detection in this image\n",
    "\n",
    "    # ...for each image in this sequence\n",
    "\n",
    "    return classifications_this_sequence\n",
    "\n",
    "# ...top_classifications_for_sequence()\n",
    "\n",
    "\n",
    "def count_above_threshold_classifications(classifications_this_sequence):\n",
    "    \"\"\"\n",
    "    Given a list of classification objects (tuples), return a dict mapping\n",
    "    category IDs to the count of above-threshold classifications.\n",
    "\n",
    "    This dict's keys will be sorted in descending order by frequency.\n",
    "    \"\"\"\n",
    "\n",
    "    # Count above-threshold classifications in this sequence\n",
    "    category_to_count = defaultdict(int)\n",
    "    for c in classifications_this_sequence:\n",
    "        if c[1] >= classification_confidence_threshold:\n",
    "            category_to_count[c[0]] += 1\n",
    "\n",
    "    # Sort the dictionary in descending order by count\n",
    "    category_to_count = {k: v for k, v in sorted(category_to_count.items(),\n",
    "                                                 key=lambda item: item[1],\n",
    "                                                 reverse=True)}\n",
    "\n",
    "    keys_sorted_by_frequency = list(category_to_count.keys())\n",
    "\n",
    "    # Handle a quirky special case: if the most common category is \"other\" and\n",
    "    # it's \"tied\" with the second-most-common category, swap them.\n",
    "    if len(other_category_names) > 0:\n",
    "        if (len(keys_sorted_by_frequency) > 1) and \\\n",
    "            (keys_sorted_by_frequency[0] in other_category_names) and \\\n",
    "            (keys_sorted_by_frequency[1] not in other_category_names) and \\\n",
    "            (category_to_count[keys_sorted_by_frequency[0]] == \\\n",
    "             category_to_count[keys_sorted_by_frequency[1]]):\n",
    "                keys_sorted_by_frequency[1], keys_sorted_by_frequency[0] = \\\n",
    "                    keys_sorted_by_frequency[0], keys_sorted_by_frequency[1]\n",
    "\n",
    "    sorted_category_to_count = {}\n",
    "    for k in keys_sorted_by_frequency:\n",
    "        sorted_category_to_count[k] = category_to_count[k]\n",
    "\n",
    "    return sorted_category_to_count\n",
    "\n",
    "# ...def count_above_threshold_classifications()\n",
    "\n",
    "def sort_images_by_time(images):\n",
    "    \"\"\"\n",
    "    Returns a copy of [images], sorted by the 'datetime' field (ascending).\n",
    "    \"\"\"\n",
    "    return sorted(images, key = lambda im: im['datetime'])\n",
    "\n",
    "\n",
    "def get_first_key_from_sorted_dictionary(di):\n",
    "    if len(di) == 0:\n",
    "        return None\n",
    "    return next(iter(di.items()))[0]\n",
    "\n",
    "\n",
    "def get_first_value_from_sorted_dictionary(di):\n",
    "    if len(di) == 0:\n",
    "        return None\n",
    "    return next(iter(di.items()))[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "137de6c1",
   "metadata": {},
   "source": [
    "## Smooth classifications at the sequence level (main loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bcceb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_other_flips = 0\n",
    "n_classification_flips = 0\n",
    "n_unclassified_flips = 0\n",
    "\n",
    "# Break if this token is contained in a filename (set to None for normal operation)\n",
    "debug_fn = None\n",
    "\n",
    "# i_sequence = 0; seq_id = all_sequences[i_sequence]\n",
    "for i_sequence,seq_id in tqdm(enumerate(all_sequences),total=len(all_sequences)):\n",
    "\n",
    "    images_this_sequence = sequence_to_images[seq_id]\n",
    "\n",
    "    # Count top-1 classifications in this sequence (regardless of confidence)\n",
    "    classifications_this_sequence = top_classifications_for_sequence(images_this_sequence)\n",
    "\n",
    "    # Handy debugging code for looking at the numbers for a particular sequence\n",
    "    for im in images_this_sequence:\n",
    "        if debug_fn is not None and debug_fn in im['file_name']:\n",
    "            raise ValueError('')\n",
    "\n",
    "    if len(classifications_this_sequence) == 0:\n",
    "        continue\n",
    "\n",
    "    # Count above-threshold classifications for each category\n",
    "    sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence)\n",
    "\n",
    "    if len(sorted_category_to_count) == 0:\n",
    "        continue\n",
    "\n",
    "    max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count)\n",
    "    dominant_category_id = get_first_key_from_sorted_dictionary(sorted_category_to_count)\n",
    "\n",
    "    # If our dominant category ID isn't something we want to smooth to, don't mess around with this sequence\n",
    "    if dominant_category_id not in category_ids_to_smooth_to:\n",
    "        continue\n",
    "\n",
    "\n",
    "    ## Smooth \"other\" classifications ##\n",
    "\n",
    "    if max_count >= min_dominant_class_classifications_above_threshold_for_other_smoothing:\n",
    "        for c in classifications_this_sequence:\n",
    "            if c[0] in other_category_ids:\n",
    "                n_other_flips += 1\n",
    "                c[0] = dominant_category_id\n",
    "                c[1] = flipped_other_confidence_value\n",
    "\n",
    "\n",
    "    # By not re-computing \"max_count\" here, we are making a decision that the count used\n",
    "    # to decide whether a class should overwrite another class does not include any \"other\"\n",
    "    # classifications we changed to be the dominant class.  If we wanted to include those...\n",
    "    #\n",
    "    # sorted_category_to_count = count_above_threshold_classifications(classifications_this_sequence)\n",
    "    # max_count = get_first_value_from_sorted_dictionary(sorted_category_to_count)\n",
    "    # assert dominant_category_id == get_first_key_from_sorted_dictionary(sorted_category_to_count)\n",
    "\n",
    "\n",
    "    ## Smooth non-dominant classes ##\n",
    "\n",
    "    if max_count >= min_dominant_class_classifications_above_threshold_for_class_smoothing:\n",
    "\n",
    "        # Don't flip classes to the dominant class if they have a large number of classifications\n",
    "        category_ids_not_to_flip = set()\n",
    "\n",
    "        for category_id in sorted_category_to_count.keys():\n",
    "            secondary_class_count = sorted_category_to_count[category_id]\n",
    "            dominant_to_secondary_ratio = max_count / secondary_class_count\n",
    "\n",
    "            # Don't smooth over this class if there are a bunch of them, and the ratio\n",
    "            # if primary to secondary class count isn't too large\n",
    "\n",
    "            # Default ratio\n",
    "            ratio_for_override = min_dominant_class_ratio_for_secondary_override_table[None]\n",
    "\n",
    "            # Does this dominant class have a custom ratio?\n",
    "            if dominant_category_id in min_dominant_class_ratio_for_secondary_override_table:\n",
    "                ratio_for_override = \\\n",
    "                    min_dominant_class_ratio_for_secondary_override_table[dominant_category_id]\n",
    "\n",
    "            if (dominant_to_secondary_ratio < ratio_for_override) and \\\n",
    "                (secondary_class_count > \\\n",
    "                 max_secondary_class_classifications_above_threshold_for_class_smoothing):\n",
    "                category_ids_not_to_flip.add(category_id)\n",
    "\n",
    "        for c in classifications_this_sequence:\n",
    "            if c[0] not in category_ids_not_to_flip and c[0] != dominant_category_id:\n",
    "                c[0] = dominant_category_id\n",
    "                c[1] = flipped_class_confidence_value\n",
    "                n_classification_flips += 1\n",
    "\n",
    "\n",
    "    ## Smooth unclassified detections ##\n",
    "\n",
    "    if max_count >= min_dominant_class_classifications_above_threshold_for_unclassified_smoothing:\n",
    "\n",
    "        results_this_sequence = results_for_sequence(images_this_sequence)\n",
    "        detections_this_sequence = []\n",
    "        for r in results_this_sequence:\n",
    "            if r['detections'] is not None:\n",
    "                detections_this_sequence.extend(r['detections'])\n",
    "        for det in detections_this_sequence:\n",
    "            if 'classifications' in det and len(det['classifications']) > 0:\n",
    "                continue\n",
    "            if det['category'] != animal_detection_category:\n",
    "                continue\n",
    "            if det['conf'] < min_detection_confidence_for_unclassified_flipping:\n",
    "                continue\n",
    "            det['classifications'] = [[dominant_category_id,flipped_unclassified_confidence_value]]\n",
    "            n_unclassified_flips += 1\n",
    "\n",
    "# ...for each sequence\n",
    "\n",
    "print('\\Finished sequence smoothing\\n')\n",
    "print('Flipped {} \"other\" classifications'.format(n_other_flips))\n",
    "print('Flipped {} species classifications'.format(n_classification_flips))\n",
    "print('Flipped {} unclassified detections'.format(n_unclassified_flips))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c826c54",
   "metadata": {},
   "source": [
    "## Write smoothed classification results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a43627f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_smoothed_classification_file = sequence_level_smoothing_input_file.replace(\n",
    "    '.json','_seqsmoothing.json')\n",
    "\n",
    "print('Writing sequence-smoothed classification results to {}'.format(\n",
    "    sequence_smoothed_classification_file))\n",
    "\n",
    "with open(sequence_smoothed_classification_file,'w') as f:\n",
    "    json.dump(d,f,indent=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "053a6fab",
   "metadata": {},
   "source": [
    "## Post-processing (post-classification, post-within-image-and-within-sequence-smoothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c074ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "options = PostProcessingOptions()\n",
    "options.image_base_dir = input_path\n",
    "options.include_almost_detections = True\n",
    "options.num_images_to_sample = 10000\n",
    "options.confidence_threshold = 0.2\n",
    "options.classification_confidence_threshold = 0.7\n",
    "options.almost_detection_confidence_threshold = options.confidence_threshold - 0.05\n",
    "options.ground_truth_json_file = None\n",
    "options.separate_detections_by_category = True\n",
    "\n",
    "options.parallelize_rendering = True\n",
    "options.parallelize_rendering_n_cores = default_workers_for_parallel_tasks\n",
    "options.parallelize_rendering_with_threads = parallelization_defaults_to_threads\n",
    "\n",
    "folder_token = sequence_smoothed_classification_file.split(os.path.sep)[-1].replace(\n",
    "    '_within_image_smoothing_seqsmoothing','')\n",
    "folder_token = folder_token.replace('.json','_seqsmoothing')\n",
    "\n",
    "output_base = os.path.join(postprocessing_output_folder, folder_token + \\\n",
    "    base_task_name + '_{:.3f}'.format(options.confidence_threshold))\n",
    "os.makedirs(output_base, exist_ok=True)\n",
    "print('Processing {} to {}'.format(base_task_name, output_base))\n",
    "\n",
    "options.api_output_file = sequence_smoothed_classification_file\n",
    "options.output_dir = output_base\n",
    "ppresults = process_batch_results(options)\n",
    "path_utils.open_file(ppresults.output_html_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eac5c71",
   "metadata": {},
   "source": [
    "## Zip .json files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed31d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_files = os.listdir(combined_api_output_folder)\n",
    "json_files = [fn for fn in json_files if fn.endswith('.json')]\n",
    "json_files = [os.path.join(combined_api_output_folder,fn) for fn in json_files]\n",
    "\n",
    "import zipfile\n",
    "from zipfile import ZipFile\n",
    "\n",
    "output_path = combined_api_output_folder\n",
    "\n",
    "def zip_json_file(fn, overwrite=False):\n",
    "\n",
    "    assert fn.endswith('.json')\n",
    "    basename = os.path.basename(fn)\n",
    "    zip_file_name = os.path.join(output_path,basename + '.zip')\n",
    "\n",
    "    if (not overwrite) and (os.path.isfile(zip_file_name)):\n",
    "        print('Skipping existing file {}'.format(zip_file_name))\n",
    "        return\n",
    "\n",
    "    print('Zipping {} to {}'.format(fn,zip_file_name))\n",
    "\n",
    "    with ZipFile(zip_file_name,'w',zipfile.ZIP_DEFLATED) as zipf:\n",
    "        zipf.write(fn,arcname=basename,compresslevel=9,compress_type=zipfile.ZIP_DEFLATED)\n",
    "\n",
    "from multiprocessing.pool import ThreadPool\n",
    "pool = ThreadPool(len(json_files))\n",
    "with tqdm(total=len(json_files)) as pbar:\n",
    "    for i,_ in enumerate(pool.imap_unordered(zip_json_file,json_files)):\n",
    "        pbar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b20c2d",
   "metadata": {},
   "source": [
    "## 99.9% of jobs end here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321b0cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Everything after this is run ad hoc and/or requires some manual editing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185bb7dc",
   "metadata": {},
   "source": [
    "## Compare results files for different model versions (or before/after RDE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a842b73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "from api.batch_processing.postprocessing.compare_batch_results import (\n",
    "    BatchComparisonOptions,PairwiseBatchComparisonOptions,compare_batch_results)\n",
    "\n",
    "options = BatchComparisonOptions()\n",
    "\n",
    "options.job_name = organization_name_short\n",
    "options.output_folder = os.path.join(postprocessing_output_folder,'model_comparison')\n",
    "options.image_folder = input_path\n",
    "\n",
    "options.pairwise_options = []\n",
    "\n",
    "filenames = [\n",
    "    '/postprocessing/organization/mdv4_results.json',\n",
    "    '/postprocessing/organization/mdv5a_results.json',\n",
    "    '/postprocessing/organization/mdv5b_results.json'\n",
    "    ]\n",
    "\n",
    "detection_thresholds = [0.7,0.15,0.15]\n",
    "\n",
    "assert len(detection_thresholds) == len(filenames)\n",
    "\n",
    "rendering_thresholds = [(x*0.6666) for x in detection_thresholds]\n",
    "\n",
    "# Choose all pairwise combinations of the files in [filenames]\n",
    "for i, j in itertools.combinations(list(range(0,len(filenames))),2):\n",
    "\n",
    "    pairwise_options = PairwiseBatchComparisonOptions()\n",
    "\n",
    "    pairwise_options.results_filename_a = filenames[i]\n",
    "    pairwise_options.results_filename_b = filenames[j]\n",
    "\n",
    "    pairwise_options.rendering_confidence_threshold_a = rendering_thresholds[i]\n",
    "    pairwise_options.rendering_confidence_threshold_b = rendering_thresholds[j]\n",
    "\n",
    "    pairwise_options.detection_thresholds_a = {'animal':detection_thresholds[i],\n",
    "                                               'person':detection_thresholds[i],\n",
    "                                               'vehicle':detection_thresholds[i]}\n",
    "    pairwise_options.detection_thresholds_b = {'animal':detection_thresholds[j],\n",
    "                                               'person':detection_thresholds[j],\n",
    "                                               'vehicle':detection_thresholds[j]}\n",
    "    options.pairwise_options.append(pairwise_options)\n",
    "\n",
    "results = compare_batch_results(options)\n",
    "\n",
    "from path_utils import open_file # from ai4eutils\n",
    "open_file(results.html_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59ff0242",
   "metadata": {},
   "source": [
    "## Merge in high-confidence detections from another results file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948f83e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.batch_processing.postprocessing.merge_detections import MergeDetectionsOptions,merge_detections\n",
    "\n",
    "source_files = ['']\n",
    "target_file = ''\n",
    "output_file = target_file.replace('.json','_merged.json')\n",
    "\n",
    "options = MergeDetectionsOptions()\n",
    "options.max_detection_size = 1.0\n",
    "options.target_confidence_threshold = 0.25\n",
    "options.categories_to_include = [1]\n",
    "options.source_confidence_thresholds = [0.2]\n",
    "merge_detections(source_files, target_file, output_file, options)\n",
    "\n",
    "merged_detections_file = output_file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1b03fb1",
   "metadata": {},
   "source": [
    "## Create a new category for large boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caebdfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.batch_processing.postprocessing import categorize_detections_by_size\n",
    "\n",
    "size_options = categorize_detections_by_size.SizeCategorizationOptions()\n",
    "\n",
    "# This is a size threshold, not a confidence threshold\n",
    "size_options.threshold = 0.9\n",
    "size_options.output_category_name = 'large_detections'\n",
    "# size_options.categories_to_separate = [3]\n",
    "size_options.measurement = 'size' # 'width'\n",
    "\n",
    "input_file = filtered_output_filename\n",
    "size_separated_file = input_file.replace('.json','-size-separated-{}.json'.format(\n",
    "    size_options.threshold))\n",
    "d = categorize_detections_by_size.categorize_detections_by_size(input_file,size_separated_file,\n",
    "                                                                size_options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4907820",
   "metadata": {},
   "source": [
    "## Preview large boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aacced3",
   "metadata": {},
   "outputs": [],
   "source": [
    "output_base_large_boxes = os.path.join(postprocessing_output_folder,\n",
    "    base_task_name + '_{}_{:.3f}_large_boxes'.format(rde_string, options.confidence_threshold))\n",
    "os.makedirs(output_base_large_boxes, exist_ok=True)\n",
    "print('Processing post-RDE, post-size-separation to {}'.format(output_base_large_boxes))\n",
    "\n",
    "options.api_output_file = size_separated_file\n",
    "options.output_dir = output_base_large_boxes\n",
    "\n",
    "ppresults = process_batch_results(options)\n",
    "html_output_file = ppresults.output_html_file\n",
    "path_utils.open_file(html_output_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a883223",
   "metadata": {},
   "source": [
    "## .json splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ebf521b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "\n",
    "from api.batch_processing.postprocessing.subset_json_detector_output import (\n",
    "    subset_json_detector_output, SubsetJsonDetectorOutputOptions)\n",
    "\n",
    "input_filename = filtered_output_filename\n",
    "output_base = os.path.join(filename_base,'json_subsets')\n",
    "\n",
    "if False:\n",
    "    if data is None:\n",
    "        with open(input_filename) as f:\n",
    "            data = json.load(f)\n",
    "    print('Data set contains {} images'.format(len(data['images'])))\n",
    "\n",
    "print('Processing file {} to {}'.format(input_filename,output_base))\n",
    "\n",
    "options = SubsetJsonDetectorOutputOptions()\n",
    "# options.query = None\n",
    "# options.replacement = None\n",
    "\n",
    "options.split_folders = True\n",
    "options.make_folder_relative = True\n",
    "\n",
    "# Reminder: 'n_from_bottom' with a parameter of zero is the same as 'bottom'\n",
    "options.split_folder_mode = 'bottom'  # 'top', 'n_from_top', 'n_from_bottom'\n",
    "options.split_folder_param = 0\n",
    "options.overwrite_json_files = False\n",
    "options.confidence_threshold = 0.01\n",
    "\n",
    "subset_data = subset_json_detector_output(input_filename, output_base, options, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aea2cc",
   "metadata": {},
   "source": [
    "## Custom splitting/subsetting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1bdbb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "\n",
    "from api.batch_processing.postprocessing.subset_json_detector_output import (\n",
    "    subset_json_detector_output, SubsetJsonDetectorOutputOptions)\n",
    "\n",
    "input_filename = filtered_output_filename\n",
    "output_base = os.path.join(filename_base,'json_subsets')\n",
    "\n",
    "folders = os.listdir(input_path)\n",
    "\n",
    "if data is None:\n",
    "    with open(input_filename) as f:\n",
    "        data = json.load(f)\n",
    "\n",
    "print('Data set contains {} images'.format(len(data['images'])))\n",
    "\n",
    "# i_folder = 0; folder_name = folders[i_folder]\n",
    "for i_folder, folder_name in enumerate(folders):\n",
    "\n",
    "    output_filename = os.path.join(output_base, folder_name + '.json')\n",
    "    print('Processing folder {} of {} ({}) to {}'.format(i_folder, len(folders), folder_name,\n",
    "          output_filename))\n",
    "\n",
    "    options = SubsetJsonDetectorOutputOptions()\n",
    "    options.confidence_threshold = 0.01\n",
    "    options.overwrite_json_files = True\n",
    "    options.query = folder_name + '/'\n",
    "\n",
    "    # This doesn't do anything in this case, since we're not splitting folders\n",
    "    # options.make_folder_relative = True\n",
    "\n",
    "    subset_data = subset_json_detector_output(input_filename, output_filename, options, data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af0372f",
   "metadata": {},
   "source": [
    "## String replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f50604",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = None\n",
    "\n",
    "from api.batch_processing.postprocessing.subset_json_detector_output import (\n",
    "    subset_json_detector_output, SubsetJsonDetectorOutputOptions)\n",
    "\n",
    "input_filename = filtered_output_filename\n",
    "output_filename = input_filename.replace('.json','_replaced.json')\n",
    "\n",
    "options = SubsetJsonDetectorOutputOptions()\n",
    "options.query = folder_name + '/'\n",
    "options.replacement = ''\n",
    "subset_json_detector_output(input_filename,output_filename,options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f4994b",
   "metadata": {},
   "source": [
    "## Splitting images into folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b4da727",
   "metadata": {},
   "outputs": [],
   "source": [
    "from api.batch_processing.postprocessing.separate_detections_into_folders import (\n",
    "    separate_detections_into_folders, SeparateDetectionsIntoFoldersOptions)\n",
    "\n",
    "default_threshold = 0.2\n",
    "base_output_folder = os.path.expanduser('~/data/{}-{}-separated'.format(base_task_name,default_threshold))\n",
    "\n",
    "options = SeparateDetectionsIntoFoldersOptions(default_threshold)\n",
    "\n",
    "options.results_file = filtered_output_filename\n",
    "options.base_input_folder = input_path\n",
    "options.base_output_folder = os.path.join(base_output_folder,folder_name)\n",
    "options.n_threads = default_workers_for_parallel_tasks\n",
    "options.allow_existing_directory = False\n",
    "\n",
    "separate_detections_into_folders(options)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b39c708",
   "metadata": {},
   "source": [
    "## Generate commands for a subset of tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b1ee03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "task_set = [8,10,12,14,16]; gpu_number = 0; sleep_time_between_tasks = 60; sleep_time_before_tasks = 0\n",
    "commands = []\n",
    "\n",
    "# i_task = 8\n",
    "for i_task in task_set:\n",
    "\n",
    "    if i_task == task_set[0]:\n",
    "        commands.append('sleep {}'.format(str(sleep_time_before_tasks)))\n",
    "\n",
    "    task = task_info[i_task]\n",
    "    chunk_file = task['input_file']\n",
    "    output_fn = chunk_file.replace('.json','_results.json')\n",
    "\n",
    "    task['output_file'] = output_fn\n",
    "\n",
    "    cuda_string = f'CUDA_VISIBLE_DEVICES={gpu_number}'\n",
    "\n",
    "    checkpoint_frequency_string = ''\n",
    "    checkpoint_path_string = ''\n",
    "    if checkpoint_frequency is not None and checkpoint_frequency > 0:\n",
    "        checkpoint_frequency_string = f'--checkpoint_frequency {checkpoint_frequency}'\n",
    "        checkpoint_path_string = '--checkpoint_path {}'.format(chunk_file.replace(\n",
    "            '.json','_checkpoint.json'))\n",
    "\n",
    "    use_image_queue_string = ''\n",
    "    if (use_image_queue):\n",
    "        use_image_queue_string = '--use_image_queue'\n",
    "\n",
    "    ncores_string = ''\n",
    "    if (ncores > 1):\n",
    "        ncores_string = '--ncores {}'.format(ncores)\n",
    "\n",
    "    quiet_string = ''\n",
    "    if quiet_mode:\n",
    "        quiet_string = '--quiet'\n",
    "\n",
    "    cmd = f'{cuda_string} python run_detector_batch.py {model_file} {chunk_file} {output_fn} {checkpoint_frequency_string} {checkpoint_path_string} {use_image_queue_string} {ncores_string} {quiet_string}'\n",
    "\n",
    "    task['command'] = cmd\n",
    "    commands.append(cmd)\n",
    "    if i_task != task_set[-1]:\n",
    "        commands.append('sleep {}'.format(str(sleep_time_between_tasks)))\n",
    "\n",
    "# ...for each task\n",
    "\n",
    "task_strings = [str(k).zfill(2) for k in task_set]\n",
    "task_set_string = '_'.join(task_strings)\n",
    "cmd_file = os.path.join(filename_base,'run_chunk_{}_gpu_{}.sh'.format(task_set_string,\n",
    "                        str(gpu_number).zfill(2)))\n",
    "\n",
    "with open(cmd_file,'w') as f:\n",
    "    for cmd in commands:\n",
    "        f.write(cmd + '\\n')\n",
    "\n",
    "import stat\n",
    "st = os.stat(cmd_file)\n",
    "os.chmod(cmd_file, st.st_mode | stat.S_IEXEC)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
